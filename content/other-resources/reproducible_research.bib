
@book{stodden_implementing_2014,
	address = {Boca Raton, FL},
	title = {Implementing reproducible research},
	isbn = {978-1-4665-6159-5 1-4665-6159-9},
	language = {English},
	publisher = {CRC Press},
	author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D},
	year = {2014},
	keywords = {bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkRAP\_QAQC, bmkRAP\_Writing, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, Floris\_ANNOTED, FlorisVanderhaeghe, THEORY},
	annote = {Computational reproducibility and (experiment) replicability; the three parts are TOOLS, PRACTICES AND GUIDELINES, PLATFORMS},
	file = {Stodden_etal_2014_Implementing_reproducible_research.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stodden_etal_2014_Implementing_reproducible_research.pdf:application/pdf;Stodden_et_al_2014_Implementing_reproducible_research.zip:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stodden_et_al_2014_Implementing_reproducible_research.zip:application/zip}
}

@article{wickham_tidy_2014,
	title = {Tidy {Data}},
	volume = {59},
	number = {10},
	journal = {Journal of Statistical Software},
	author = {Wickham, Hadley},
	year = {2014},
	keywords = {bmkSOF\_Databanken, bmkSOF\_R, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, FlorisVanderhaeghe, THEORY},
	annote = {How to make tidy data},
	file = {Wickham_2014_JStatSoft.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wickham_2014_JStatSoft.pdf:application/pdf}
}

@book{gunther_learn_2014,
	title = {Learn {Version} {Control} with {Git}: {A} step-by-step course for the complete beginner},
	volume = {1},
	author = {Günther, Tobias},
	month = apr,
	year = {2014},
	keywords = {bmkADM\_BibVanBMK, bmkSOF\_Andere, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, ESSENTIAL\_reading\_Floris, Floris\_EBOOK, THEORY}
}

@article{stodden_best_2014,
	title = {Best {Practices} for {Computational} {Science}: {Software} {Infrastructure} and {Environments} for {Reproducible} and {Extensible} {Research}},
	volume = {2},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2049-9647},
	shorttitle = {Best {Practices} for {Computational} {Science}},
	url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/},
	doi = {10.5334/jors.ay},
	abstract = {The goal of this article is to coalesce a discussion around best practices for scholarly research that utilizes computational methods, by providing a formalized set of best practice recommendations to guide computational scientists and other stakeholders wishing to disseminate reproducible research, facilitate innovation by enabling data and code re-use, and enable broader communication of the output of computational scientific research. Scholarly dissemination and communication standards are changing to reflect the increasingly computational nature of scholarly research, primarily to include the sharing of the data and code associated with published results. We also present these Best Practices as a living, evolving, and changing document at http://wiki.stodden.net/Best\_Practices.},
	language = {en},
	number = {1},
	urldate = {2016-02-26},
	journal = {Journal of Open Research Software},
	author = {Stodden, Victoria and Miguez, Sheila},
	month = jul,
	year = {2014},
	keywords = {archiving, best practices, bmkSOF\_Andere, bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, code sharing, computational science, data sharing, ESSENTIAL\_reading\_Floris, open science, reproducible research, scientific method, THEORY, wiki},
	annote = {A formalized set of best practice recommendations for reproducible research},
	file = {Stodden_Miguez_2014_JOpenResSoftw.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stodden_Miguez_2014_JOpenResSoftw.pdf:application/pdf}
}

@article{wilson_good_2016,
	title = {Good {Enough} {Practices} in {Scientific} {Computing}},
	url = {http://arxiv.org/abs/1609.00037},
	urldate = {2016-09-06},
	journal = {PLOS Submission},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
	year = {2016},
	keywords = {bmkSOF\_Andere, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	annote = {Computing tools and techniques for reproducible science},
	file = {Wilson_etal_2016_PLOS.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wilson_etal_2016_PLOS.pdf:application/pdf}
}

@article{wilson_good_2017,
	title = {Good enough practices in scientific computing},
	volume = {13},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510},
	doi = {10.1371/journal.pcbi.1005510},
	abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
	number = {6},
	urldate = {2017-08-24},
	journal = {PLOS Computational Biology},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
	month = jun,
	year = {2017},
	keywords = {bmkANA\_Andere, bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkRAP\_Writing, bmkSOF\_Andere, bmkSOF\_Databanken, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ModellingAndStatistics, CAT\_Publishing, CAT\_ResearchStrategyAndTechniques, Computer software, Control systems, Data management, Data processing, ESSENTIAL\_reading\_Floris, Programming languages, reproducibility, Software tools, Source code, THEORY},
	pages = {e1005510},
	annote = {Set of good computing practices that every researcher can adopt},
	file = {Wilson_etal_2017_PLOSComputBiol.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wilson_etal_2017_PLOSComputBiol.pdf:application/pdf}
}

@article{lowndes_our_2017,
	title = {Our path to better science in less time using open data science tools},
	volume = {1},
	copyright = {2017 Nature Publishing Group},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-017-0160},
	doi = {10.1038/s41559-017-0160},
	abstract = {{\textless}p{\textgreater}Reproducibility starts with having a transparent and streamlined workflow. Here, the authors describe how they achieved this using open data tools for the collaborative Ocean Health Index project.{\textless}/p{\textgreater}},
	language = {en},
	number = {6},
	urldate = {2017-08-24},
	journal = {Nature Ecology \& Evolution},
	author = {Lowndes, Julia S. Stewart and Best, Benjamin D. and Scarborough, Courtney and Afflerbach, Jamie C. and Frazier, Melanie R. and O’Hara, Casey C. and Jiang, Ning and Halpern, Benjamin S.},
	month = may,
	year = {2017},
	keywords = {bmkANA\_Andere, bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkRAP\_Writing, bmkSOF\_Andere, bmkSOF\_Databanken, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ModellingAndStatistics, CAT\_Publishing, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {s41559--017--0160--017},
	annote = {Learning open data science tools},
	file = {Lowndes_etal_2017_NatureEcolEvol.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/L/Lowndes_etal_2017_NatureEcolEvol.pdf:application/pdf}
}

@article{baker_is_2016,
	title = {Is there a reproducibility crisis?},
	volume = {533},
	url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	number = {7604},
	urldate = {2017-09-25},
	journal = {Nature},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	keywords = {bmkCTX\_BeleidsgerichteMeetnetten, bmkCTX\_Onderzoekscontext, bmkFAS\_2OntwerpObservatie, bmkFAS\_3OntwerpAnalyse, bmkSOF\_Andere, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {452--454},
	annote = {Results from a Nature survey on research reproducibility},
	file = {Baker_2016_Nature.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/Baker_2016_Nature.pdf:application/pdf}
}

@article{begley_institutions_2015,
	title = {Institutions must do their part for reproducibility},
	volume = {525},
	shorttitle = {Robust research},
	url = {http://www.nature.com/news/robust-research-institutions-must-do-their-part-for-reproducibility-1.18259},
	doi = {10.1038/525025a},
	abstract = {Tie funding to verified good institutional practice, and robust science will shoot up the agenda, say C. Glenn Begley, Alastair M.},
	number = {7567},
	urldate = {2017-09-25},
	journal = {Nature},
	author = {Begley, C. Glenn and Buchan, Alastair M. and Dirnagl, Ulrich},
	month = sep,
	year = {2015},
	keywords = {bmkCTX\_BeleidsgerichteMeetnetten, bmkCTX\_Onderzoekscontext, bmkFAS\_2OntwerpObservatie, bmkFAS\_3OntwerpAnalyse, bmkRAP\_QAQC, bmkSOF\_Andere, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {25--27},
	annote = {Current irreproducibility and good institutional practice},
	file = {Begley_etal_2015_Nature.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/Begley_etal_2015_Nature.pdf:application/pdf}
}

@article{collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" src="https://d2ufo47lrtsv5s.cloudfront.net/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2017-09-25},
	journal = {Science},
	author = {Collaboration, Open Science},
	month = aug,
	year = {2015},
	pmid = {26315443},
	keywords = {bmkFAS\_2OntwerpObservatie, bmkFAS\_3OntwerpAnalyse, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {aac4716},
	annote = {Results of reproducing 100 published experiments}
}

@incollection{ibanez_practicing_2014,
	address = {Boca Raton, FL},
	title = {Practicing open science},
	isbn = {978-1-4665-6159-5 1-4665-6159-9},
	language = {English},
	booktitle = {Implementing reproducible research},
	publisher = {CRC Press},
	author = {Ibanez, Luis and Schroeder, William J. and Hanwell, Marcus D.},
	editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D},
	year = {2014},
	keywords = {bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkRAP\_QAQC, bmkRAP\_Writing, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, Floris\_ANNOTED, FlorisVanderhaeghe, THEORY},
	annote = {Reproducible science: vision, routine practices, collaboration, literate computing},
	file = {Ibanez_etal_2014_Implementing_reproducible_research.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/I/Ibanez_etal_2014_Implementing_reproducible_research.pdf:application/pdf}
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	url = {http://dx.doi.org/10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	keywords = {bmkANA\_Andere, bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkRAP\_Writing, bmkSOF\_Andere, bmkSOF\_Databanken, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ModellingAndStatistics, CAT\_Publishing, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {160018},
	file = {Wilkinson_etal_2016_SciData.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wilkinson_etal_2016_SciData.pdf:application/pdf}
}

@article{smith_software_2016,
	title = {Software citation principles},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-86},
	doi = {10.7717/peerj-cs.86},
	abstract = {Software is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation. Inspired by the activities of the FORCE11 working group focused on data citation, this document summarizes the recommendations of the FORCE11 Software Citation Working Group and its activities between June 2015 and April 2016. Based on a review of existing community practices, the goal of the working group was to produce a consolidated set of citation principles that may encourage broad adoption of a consistent policy for software citation across disciplines and venues. Our work is presented here as a set of software citation principles, a discussion of the motivations for developing the principles, reviews of existing community practice, and a discussion of the requirements these principles would place upon different stakeholders. Working examples and possible technical solutions for how these principles can be implemented will be discussed in a separate paper.},
	language = {en},
	urldate = {2018-02-19},
	journal = {PeerJ Computer Science},
	author = {Smith, Arfon M. and Katz, Daniel S. and Niemeyer, Kyle E.},
	month = sep,
	year = {2016},
	keywords = {bmkFAS\_4OntwerpRapportering, bmkRAP\_Writing, bmkSOF\_Andere, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_Publishing, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {e86},
	annote = {Recommendations of the FORCE11 Software Citation Working Group},
	file = {Smith_etal_2016_PeerJCompSci.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Smith_etal_2016_PeerJCompSci.pdf:application/pdf}
}

@article{culina_navigating_2018,
	title = {Navigating the unfolding open data landscape in ecology and evolution},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-017-0458-2},
	doi = {10.1038/s41559-017-0458-2},
	abstract = {Open data is increasing rapidly, but data sets may be scattered among many repositories. Here, the authors present an overview of the open data landscape in ecology and evolutionary biology, and highlight key points to consider when reusing data.},
	language = {en},
	number = {3},
	urldate = {2018-02-28},
	journal = {Nature Ecology \& Evolution},
	author = {Culina, Antica and Baglioni, Miriam and Crowther, Tom W. and Visser, Marcel E. and Woutersen-Windhouwer, Saskia and Manghi, Paolo},
	month = mar,
	year = {2018},
	keywords = {bmkCTX\_Onderzoekscontext, bmkSOF\_Andere, bmkSOF\_Databanken, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, ESSENTIAL\_reading\_Floris},
	pages = {420--426},
	annote = {Overview of online data infrastructures and considerations to be made},
	file = {Culina_etal_2018_NatureEcolEvol.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/C/Culina_etal_2018_NatureEcolEvol.pdf:application/pdf}
}

@article{hampton_tao_2015,
	title = {The {Tao} of open science for ecology},
	volume = {6},
	issn = {2150-8925},
	url = {http://onlinelibrary.wiley.com/doi/10.1890/ES14-00402.1/abstract},
	doi = {10.1890/ES14-00402.1},
	abstract = {The field of ecology is poised to take advantage of emerging technologies that facilitate the gathering, analyzing, and sharing of data, methods, and results. The concept of transparency at all stages of the research process, coupled with free and open access to data, code, and papers, constitutes “open science.” Despite the many benefits of an open approach to science, a number of barriers to entry exist that may prevent researchers from embracing openness in their own work. Here we describe several key shifts in mindset that underpin the transition to more open science. These shifts in mindset include thinking about data stewardship rather than data ownership, embracing transparency throughout the data life-cycle and project duration, and accepting critique in public. Though foreign and perhaps frightening at first, these changes in thinking stand to benefit the field of ecology by fostering collegiality and broadening access to data and findings. We present an overview of tools and best practices that can enable these shifts in mindset at each stage of the research process, including tools to support data management planning and reproducible analyses, strategies for soliciting constructive feedback throughout the research process, and methods of broadening access to final research products.},
	language = {en},
	number = {7},
	journal = {Ecosphere},
	author = {Hampton, Stephanie E. and Anderson, Sean S. and Bagby, Sarah C. and Gries, Corinna and Han, Xueying and Hart, Edmund M. and Jones, Matthew B. and Lenhardt, W. Christopher and MacDonald, Andrew and Michener, William K. and Mudge, Joe and Pourmokhtarian, Afshin and Schildhauer, Mark P. and Woo, Kara H. and Zimmerman, Naupaka},
	month = jul,
	year = {2015},
	keywords = {bmkFAS\_2OntwerpObservatie, bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkSOF\_Programming, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, Data management, Ecology, ESSENTIAL\_reading\_Floris, open access, open science, reproducible research, THEORY},
	pages = {1--13},
	annote = {Workflows, tools, obstacles and needed mindshifts for open science},
	file = {Hampton_etal_2015_Ecosphere.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/H/Hampton_etal_2015_Ecosphere.pdf:application/pdf}
}

@article{stevens_building_2018,
	title = {Building a local community of practice in scientific programming for {Life} {Scientists}},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/02/15/265421},
	doi = {10.1101/265421},
	abstract = {For most experimental biologists, handling the avalanche of data generated is similar to self-learn how to drive. Although that might be doable, it is preferable and safer to learn good practices. One way to achieve this is to build local communities of practice by bringing together scientists that perform code-intensive research to spread know-how and good practices. Here, we indicate important challenges and issues that stand in the way of establishing these local communities of practice. For a given researcher working for an academic institution, their capacity to conduct data-intensive research will be arbitrarily relying on the presence of well-trained bioinformaticians in their neighborhood. In this paper, we propose a model to build a local community of practice for scientific programmers. First, Software/Data Carpentry (SWC) programming workshops designed for researchers new to computational biology can be organized. However, while they provide an immediate solution for learning, more regular long-term assistance is also needed. Researchers need persisting, local support to continue learning and to solve programming issues that hamper their research progress. The solution we describe here is to implement a study group where researchers can meet-up and help each other in a "safe-learning atmosphere". Based on our experience, we describe two examples of building local communities of practice: one in the Netherlands at the Amsterdam Science Park and one in the United States at the University of Wisconsin-Madison. The current challenge is to make these local communities self-sustainable despite the high turnover of researchers at any institution and the lack of academic reward (e.g. publication). Here, we present some lessons learned from our experience. We believe that our local communities of practice will prove useful for other scientists that want to set up similar structures of researchers involved in scientific programming and data science.},
	language = {en},
	urldate = {2018-02-28},
	journal = {bioRxiv},
	author = {Stevens, Sarah L. R. and Kuzak, Mateusz and Martinez, Carlos and Moser, Aurelia and Bleeker, Petra M. and Galland, Marc},
	month = feb,
	year = {2018},
	keywords = {bmkSOF\_Programming, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, THEORY},
	pages = {265421},
	annote = {Local community of practice for scientific programming: why, how (with scheme), challenges},
	file = {Stevens_etal_2018_bioRxiv.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stevens_etal_2018_bioRxiv.pdf:application/pdf}
}

@article{hampton_skills_2017,
	title = {Skills and {Knowledge} for {Data}-{Intensive} {Environmental} {Research}},
	volume = {67},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/67/6/546/3784601},
	doi = {10.1093/biosci/bix025},
	abstract = {The scale and magnitude of complex and pressing environmental issues lend urgency to the need for integrative and reproducible analysis and synthesis, facilitated by data-intensive research approaches. However, the recent pace of technological change has been such that appropriate skills to accomplish data-intensive research are lacking among environmental scientists, who more than ever need greater access to training and mentorship in computational skills. Here, we provide a roadmap for raising data competencies of current and next-generation environmental researchers by describing the concepts and skills needed for effectively engaging with the heterogeneous, distributed, and rapidly growing volumes of available data. We articulate five key skills: (1) data management and processing, (2) analysis, (3) software skills for science, (4) visualization, and (5) communication methods for collaboration and dissemination. We provide an overview of the current suite of training initiatives available to environmental scientists and models for closing the skill-transfer gap.},
	language = {en},
	number = {6},
	urldate = {2018-02-28},
	journal = {BioScience},
	author = {Hampton, Stephanie E. and Jones, Matthew B. and Wasser, Leah A. and Schildhauer, Mark P. and Supp, Sarah R. and Brun, Julien and Hernandez, Rebecca R. and Boettiger, Carl and Collins, Scott L. and Gross, Louis J. and Fernández, Denny S. and Budden, Amber and White, Ethan P. and Teal, Tracy K. and Labou, Stephanie G. and Aukema, Juliann E.},
	month = jun,
	year = {2017},
	keywords = {bmkANA\_Andere, bmkRAP\_Writing, bmkSOF\_Andere, bmkSOF\_Databanken, bmkSOF\_Programming, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ModellingAndStatistics, CAT\_ResearchStrategyAndTechniques, THEORY},
	pages = {546--557},
	annote = {Training approaches and needed skills in data science},
	file = {Hampton_etal_2017_BioScience.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/H/Hampton_etal_2017_BioScience.pdf:application/pdf}
}

@article{donati_information_2017,
	title = {Information management: {Data} domination},
	volume = {548},
	copyright = {© 2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	shorttitle = {Information management},
	url = {https://www.nature.com/nature/journal/v548/n7669/full/nj7669-613a.html?foxtrotcallback=true},
	doi = {10.1038/nj7669-613a},
	abstract = {Software programming, algorithm development and other technological skills can give scientists an edge in their fields.},
	language = {en},
	number = {7669},
	urldate = {2018-02-28},
	journal = {Nature},
	author = {Donati, Gaia and Woolston, Chris},
	month = aug,
	year = {2017},
	keywords = {bmkANA\_Andere, bmkSOF\_Andere, bmkSOF\_Databanken, bmkSOF\_Programming, Business, Careers, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ModellingAndStatistics, CAT\_ResearchStrategyAndTechniques, Information technology, THEORY},
	pages = {613--614},
	annote = {How data science is becoming a large discipline},
	file = {Donati_Woolston_2017_Nature.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/D/Donati_Woolston_2017_Nature.pdf:application/pdf}
}

@article{perkel_democratic_2016,
	title = {Democratic databases: science on {GitHub}},
	volume = {538},
	shorttitle = {Democratic databases},
	url = {http://www.nature.com/news/democratic-databases-science-on-github-1.20719},
	doi = {10.1038/538127a},
	abstract = {Scientists are turning to a software–development site to share data and code.},
	language = {en},
	number = {7623},
	urldate = {2018-02-28},
	journal = {Nature News},
	author = {Perkel, Jeffrey},
	month = oct,
	year = {2016},
	keywords = {bmkFAS\_2OntwerpObservatie, bmkSOF\_Andere, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, THEORY},
	pages = {127},
	annote = {Overview of git, github and some data repository sites},
	file = {Perkel_2016_NatureNews.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/P/Perkel_2016_NatureNews.pdf:application/pdf}
}

@article{kaplan_teaching_2017,
	title = {Teaching stats for data science},
	volume = {5},
	url = {https://peerj.com/preprints/3205},
	doi = {10.7287/peerj.preprints.3205v1},
	abstract = {The familiar mathematical topics of introductory statistics --- means, proportions, t-tests, normal and t distributions, chi-squared, etc. --- are a product of the first half of the 20th century. Naturally, they reflect the statistical conditions of that era: scarce, e.g.
            
            \&lt; 10, data originating in benchtop or agricultural experiments; algorithms communicated via algebraic formulas. Today, applied statistics relates to a different environment: software is the means of algorithmic communication, observational and "unplanned" data are interpreted for causal relationships, and data are large both in
            
            and the number of variables. This change in situation calls for a thorough rethinking of the topics in and approach to statistics education. This paper presents a set of ten organizing blocks for intro stats that are better suited to today's environment.},
	journal = {PeerJ Preprints},
	author = {Kaplan, Daniel T},
	year = {2017},
	keywords = {bmkANA\_Andere, CAT\_ModellingAndStatistics, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {e3205v1},
	annote = {Ten organizing blocks for introductory statistics teaching, in the present data science context},
	file = {Kaplan_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/K/Kaplan_2017_PeerJPreprints.pdf:application/pdf}
}

@article{bryan_excuse_2017,
	title = {Excuse me, do you have a moment to talk about version control?},
	volume = {5},
	url = {https://peerj.com/preprints/3159},
	doi = {10.7287/peerj.preprints.3159v2},
	abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
	journal = {PeerJ Preprints},
	author = {Bryan, Jennifer},
	year = {2017},
	keywords = {bmkSOF\_Programming, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, THEORY},
	pages = {e3159v2},
	annote = {Rationale, workflows and tools regarding version control for project organization},
	file = {Bryan_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/Bryan_2017_PeerJPreprints.pdf:application/pdf}
}

@article{cetinkaya-rundel_infrastructure_2017,
	title = {Infrastructure and tools for teaching computing throughout the statistical curriculum},
	volume = {5},
	url = {https://peerj.com/preprints/3181},
	doi = {10.7287/peerj.preprints.3181v1},
	abstract = {Modern statistics is fundamentally a computational discipline, but too often this fact is not reflected in our statistics curricula. With the rise of big data and data science it has become increasingly clear that students both want, expect, and need explicit training in this area of the discipline. Additionally, recent curricular guidelines clearly state that working with data requires extensive computing skills and that statistics students should be fluent in accessing, manipulating, analyzing, and modeling with professional statistical analysis software. Much has been written in the statistics education literature about pedagogical tools and approaches to provide a practical computational foundation for students. This article discusses the computational infrastructure and toolkit choices to allow for these pedagogical innovations while minimizing frustration and improving adoption for both our students and instructors.},
	journal = {PeerJ Preprints},
	author = {Cetinkaya-Rundel, Mine and Rundel, Colin W},
	year = {2017},
	keywords = {bmkANA\_Andere, bmkSOF\_Programming, CAT\_ICT\_SoftwareAndProgramming, CAT\_ModellingAndStatistics, THEORY},
	pages = {e3181v1},
	annote = {Computational infrastructure and toolkit choices to allow for the necessary pedagogical innovations in statistics education},
	file = {Cetinkaya-Rundel_Rundel_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/C/Cetinkaya-Rundel_Rundel_2017_PeerJPreprints.pdf:application/pdf}
}

@article{ellis_how_2017,
	title = {How to share data for collaboration},
	volume = {5},
	url = {https://peerj.com/preprints/3139},
	doi = {10.7287/peerj.preprints.3139v5},
	abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
	journal = {PeerJ Preprints},
	author = {Ellis, Shannon E and Leek, Jeffrey T},
	year = {2017},
	keywords = {bmkSOF\_Databanken, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {e3139v5},
	annote = {Guidelines for providing data to a scientist: provide raw data, format consistently, include metadata \& preprocessing steps},
	file = {Ellis_Leek_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/E/Ellis_Leek_2017_PeerJPreprints.pdf:application/pdf}
}

@article{mcnamara_wrangling_2017,
	title = {Wrangling categorical data in {R}},
	volume = {5},
	url = {https://peerj.com/preprints/3163},
	doi = {10.7287/peerj.preprints.3163v2},
	abstract = {Data wrangling is a critical foundation of data science, and wrangling of categorical data is an important component of this process. However, categorical data can introduce unique issues in data wrangling, particularly in real-world settings with collaborators and periodically-updated dynamic data. This paper discusses common problems arising from categorical variable transformations in R, demonstrates the use of factors, and suggests approaches to address data wrangling challenges. For each problem, we present at least two strategies for management, one in base R and the other from the ‘tidyverse.’ We consider several motivating examples, suggest defensive coding strategies, and outline principles for data wrangling to help ensure data quality and sound analysis.},
	journal = {PeerJ Preprints},
	author = {McNamara, Amelia and Horton, Nicholas J},
	year = {2017},
	keywords = {bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {e3163v2},
	annote = {Working with factors in R: tidyverse vs base R},
	file = {McNamara_Horton_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/M/McNamara_Horton_2017_PeerJPreprints.pdf:application/pdf}
}

@article{ross_declutter_2017,
	title = {Declutter your {R} workflow with tidy tools},
	volume = {5},
	url = {https://peerj.com/preprints/3180},
	doi = {10.7287/peerj.preprints.3180v1},
	abstract = {The R language has withstood the test of time. Forty years after it was initially developed (in the form of the S language) R is being used by millions of programmers on workflows the inventors of the language could never have imagined. Although base R packages perform well in most settings, workflows can be made more efficient by developing packages with more consistent arguments, inputs and outputs and emphasizing constantly improving code over historical code consistency. The universe of R packages known as the tidyverse, including dplyr, tidyr and others, aim to improve workflows and make data analysis as smooth as possible by applying a set of core programming principles in package development.},
	journal = {PeerJ Preprints},
	author = {Ross, Zev and Wickham, Hadley and Robinson, David},
	year = {2017},
	keywords = {bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, ESSENTIAL\_reading\_Floris, THEORY},
	pages = {e3180v1},
	annote = {Philosophy of tidyverse},
	file = {Ross_etal_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/R/Ross_etal_2017_PeerJPreprints.pdf:application/pdf}
}

@book{cooper_guide_2017,
	address = {London},
	series = {{BES} {Guides} to {Better} {Science}},
	title = {A guide to reproducible code in ecology and evolution},
	publisher = {British Ecological Society},
	editor = {Cooper, Natalie and Hsing, Pen-Yuan},
	year = {2017},
	keywords = {bmkFAS\_3OntwerpAnalyse, bmkFAS\_4OntwerpRapportering, bmkFAS\_5Implementatie, bmkRAP\_QAQC, bmkRAP\_Writing, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_SoftwareAndProgramming, CAT\_Publishing, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, REVIEW},
	annote = {File organisation, workflow documentation, code reproducibility and readability, writing reproducible reports, version control and code archiving},
	file = {Cooper_Hsing_2017_A_guide_to_reproducible_code_in_ecology_and_evolution.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/C/Cooper_Hsing_2017_A_guide_to_reproducible_code_in_ecology_and_evolution.pdf:application/pdf}
}

@book{british_ecological_society_guide_2014,
	address = {London},
	series = {{BES} {Guides} to {Better} {Science}},
	title = {A guide to data management in ecology and evolution},
	publisher = {British Ecological Society},
	editor = {{British Ecological Society}},
	year = {2014},
	keywords = {bmkFAS\_2OntwerpObservatie, bmkFAS\_5Implementatie, bmkRAP\_QAQC, bmkSOF\_Databanken, bmkSOF\_Programming, bmkSOF\_R, CAT\_ICT\_Database, CAT\_ICT\_SoftwareAndProgramming, CAT\_Publishing, CAT\_ResearchStrategyAndTechniques, ESSENTIAL\_reading\_Floris, REVIEW},
	annote = {Planning the data life cycle; creating, processing, documenting, preserving, sharing \& reusing data},
	file = {British_Ecological_Society_2014_A_guide_to_data_management_in_ecology_and_evolution.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/British_Ecological_Society_2014_A_guide_to_data_management_in_ecology_and_evolution.pdf:application/pdf}
}