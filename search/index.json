[{"content":" Welcome to the INBO tutorial pages\n ","href":"/tutorials/","title":"Home"},{"content":"","href":"/tutorials/articles/","title":"Articles"},{"content":"","href":"/tutorials/tutorials/","title":"Tutorials"},{"content":" Create and add a tutorial First of all, thanks to consider making a new tutorial\u0026hellip;\nTODO De idee:\n maak Rmd (met bepaalde headers,\u0026hellip;) -\u0026gt; genereer zelf .md versie (- Rmd wordt niet bekeken door blogdown/hugo) alle md\u0026rsquo;s -\u0026gt; website  update van de search, node ./themes/minimo/scripts/generate-search-index-lunr.js (! dat script nog te verplaatsen, zodat afzonderlijk staat)  voor files waarbij interactieve componenten: blogdown gebruiken  https://owi.usgs.gov/blog/leaflet/ https://www.bryanwhiting.com/2018/07/debugging-leaflet-and-googlesheets-on-blogdown/ https://stackoverflow.com/questions/53464336/hugo-relative-paths-in-page-bundles\nTODO: - travis build script for hugo build - interactive maps solution - tutorial about how to tutorial - check script if for each Rmd an equivalent md exists\u0026hellip; (use R/build.R?)\nHeader structure markdown file  --- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; author: \u0026quot;YOUR NAME\u0026quot; date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;first_tag\u0026quot;, \u0026quot;second_tag\u0026quot;, \u0026quot;...\u0026quot;] --- # your tutorial starts here...  Rmarkdown file --- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; author: \u0026quot;YOUR NAME\u0026quot; date: YYYY-MM-DD categories: - YOUR_CATEGORY tags: [\u0026quot;first_tag\u0026quot;, \u0026quot;second_tag\u0026quot;, \u0026quot;...\u0026quot;] output: md_document: preserve_yaml: true --- # your tutorial starts here...  ","href":"/tutorials/create_tutorial/","title":"Create tutorial"},{"content":"","href":"/tutorials/categories/","title":"Categories"},{"content":"  McElreath (2015): Statistical Rethinking is an introduction to applied Bayesian data analysis, aimed at PhD students and researchers in the natural and social sciences. This audience has had some calculus and linear algebra, and one or two joyless undergraduate courses in statistics. I\u0026rsquo;ve been teaching applied statistics to this audience for about a decade now, and this book has evolved from that experience. The book teaches generalized linear multilevel modeling (GLMMs) from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. The book covers the basics of regression through multilevel models, as well as touching on measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. This is not a traditional mathematical statistics book. Instead the approach is computational, using complete R code examples, aimed at developing skilled and skeptical scientists. Theory is explained through simulation exercises, using R code. And modeling examples are fully worked, with R code displayed within the main text. Mathematical depth is given in optional {\u0026ldquo;}overthinking{\u0026rdquo;} boxes throughout.\n Kass et al. (2016): The authors propose a set of 10 simple rules for effective statistical practice\n Quinn \u0026amp; Keough (2002): An essential textbook for any student or researcher in biology needing to design experiments, sample programs or analyse the resulting data. The text begins with a revision of estimation and hypothesis testing methods, covering both classical and Bayesian philosophies, before advancing to the analysis of linear and generalized linear models. Topics covered include linear and logistic regression, simple and complex ANOVA models (for factorial, nested, block, split-plot and repeated measures and covariance designs), and log-linear models. Multivariate techniques, including classification and ordination, are then introduced. Special emphasis is placed on checking assumptions, exploratory data analysis and presentation of results. The main analyses are illustrated with many examples from published papers and there is an extensive reference list to both the statistical and biological literature. The book is supported by a website that provides all data sets, questions for each chapter and links to software.\n James et al. (2013): An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.\n Emden (2008): The typical biology student is “hardwired” to be wary of any tasks involving the application of mathematics and statistical analyses, but the plain fact is much of biology requires interpretation of experimental data through the use of statistical methods. This unique textbook aims to demystify statistical formulae for the average biology student. Written in a lively and engaging style, Statistics for Terrified Biologists draws on the author\u0026rsquo;s 30 years of lecturing experience. One of the foremost entomologists of his generation, van Emden has an extensive track record for successfully teaching statistical methods to even the most guarded of biology students. For the first time basic methods are presented using straightforward, jargon-free language. Students are taught to use simple formulae accurately to interpret what is being measured with each test and statistic, while at the same time learning to recognize overall patterns and guiding principles. Complemented by simple illustrations and useful case studies, this is an ideal statistics resource tool for undergraduate biology and environmental science students who lack confidence in their mathematical abilities.\n Agresti (2002): The use of statistical methods for categorical data has increased dramatically, particularly for applications in the biomedical and social sciences. Responding to new developments in the field as well as to the needs of a new generation of professionals and students, this new edition of the classic Categorical Data Analysis offers a comprehensive introduction to the most important methods for categorical data analysis. Designed for statisticians and biostatisticians as well as scientists and graduate students practicing statistics, Categorical Data Analysis, Second Edition summarizes the latest methods for univariate and correlated multivariate categorical responses. Readers will find a unified generalized linear models approach that connects logistic regression and Poisson and negative binomial regression for discrete data with normal regression for continuous data.\n van Belle (2008): This book contains chapters titled:\n Begin with a Basic Formula for Sample Size–Lehr\u0026rsquo;s Equation Calculating Sample Size Using the Coefficient of Variation Ignore the Finite Population Correction in Calculating Sample Size for a Survey The Range of the Observations Provides Bounds for the Standard Deviation * Do not Formulate a Study Solely in Terms of Effect Size Overlapping Confidence Intervals do not Imply Nonsignificance Sample Size Calculation for the Poisson Distribution Sample Size Calculation for Poisson Distribution with Background Rate Sample Size Calculation for the Binomial Distribution When Unequal Sample Sizes Matter; When They Don\u0026rsquo;t * Determining Sample Size when there are Different Costs Associated with the Two Samples Use the Rule of Threes for 95% Upper Bounds when there Have Been No Events Sample Size Calculations Should be Based on the Way the Data will be Analyzed  Grolemund \u0026amp; Wickham (2016): This is the website for {\u0026ldquo;}R for Data Science{\u0026rdquo;}. This book will teach you how to do data science with R: You\u0026rsquo;ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you\u0026rsquo;ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You\u0026rsquo;ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You\u0026rsquo;ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\n Baddeley et al. (2015): Spatial Point Patterns: Methodology and Applications with R shows scientific researchers and applied statisticians from a wide range of fields how to analyze their spatial point pattern data. Making the techniques accessible to non-mathematicians, the authors draw on their 25 years of software development experiences, methodological research, and broad scientific collaborations to deliver a book that clearly and succinctly explains concepts and addresses real scientific questions. Practical Advice on Data Analysis and Guidance on the Validity and Applicability of Methods The first part of the book gives an introduction to R software, advice about collecting data, information about handling and manipulating data, and an accessible introduction to the basic concepts of point processes. The second part presents tools for exploratory data analysis, including non-parametric estimation of intensity, correlation, and spacing properties. The third part discusses model-fitting and statistical inference for point patterns. The final part describes point patterns with additional {\u0026ldquo;}structure,{\u0026rdquo;} such as complicated marks, space-time observations, three- and higher-dimensional spaces, replicated observations, and point patterns constrained to a network of lines. Easily Analyze Your Own Data Throughout the book, the authors use their spatstat package, which is free, open-source code written in the R language. This package provides a wide range of capabilities for spatial point pattern data, from basic data handling to advanced analytic tools. The book focuses on practical needs from the user\u0026rsquo;s perspective, offering answers to the most frequently asked questions in each chapter.\n Hobbs \u0026amp; Hooten (2015): Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods—in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach. Bayesian Models is an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals. This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.\n Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticians Covers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and more - Deemphasizes computer coding in favor of basic principles Explains how to write out properly factored statistical expressions representing Bayesian models  Zuur et al. (2017): In Volume I we explain how to apply linear regression models, generalised linear models (GLM), and generalised linear mixed-effects models (GLMM) to spatial, temporal, and spatial-temporal data. The models that will be employed use the Gaussian and gamma distributions for continuous data, the Poisson and negative binomial distributions for count data, the Bernoulli distribution for absence–presence data, and the binomial distribution for proportional data.In Volume II we apply zero-inflated models and generalised additive (mixed-effects) models to spatial and spatial-temporal data. We also discuss models with more exotic distributions like the generalised Poisson distribution to deal with underdispersion and the beta distribution to analyse proportional data.\n Zuur et al. (2010):\n While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a randomsample of theirwork (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniquesemployed. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially trouble- somein applied ecology, wheremanagement and policy decisions are often at stake. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance ofmaking wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses. Key-words:  Kelleher \u0026amp; Wagener (2011): Our ability to visualize scientific data has evolved significantly over the last 40 years. However, this advancement does not necessarily alleviate many common pitfalls in visualization for scientific journals, which can inhibit the ability of readers to effectively understand the information presented. To address this issue within the context of visualizing environmental data, we list ten guidelines for effective data visualization in scientific publications. These guidelines support the primary objective of data visualization, i.e. to effectively convey information. We believe that this small set of guidelines based on a review of key visualization literature can help researchers improve the communication of their results using effective visualization. Enhancement of environmental data visualization will further improve research presentation and communication within and across disciplines.\n Lohr (2010): Sharon L. Lohr\u0026rsquo;s SAMPLING: DESIGN AND ANALYSIS, 2ND EDITION, provides a modern introduction to the field of survey sampling intended for a wide audience of statistics students. Practical and authoritative, the book is listed as a standard reference for training on real-world survey problems by a number of prominent surveying organizations. Lohr concentrates on the statistical aspects of taking and analyzing a sample, incorporating a multitude of applications from a variety of disciplines. The text gives guidance on how to tell when a sample is valid or not, and how to design and analyze many different forms of sample surveys. Recent research on theoretical and applied aspects of sampling is included, as well as optional technology instructions for using statistical software with survey data.\n Zuur et al. (2009): Building on the successful Analysing Ecological Data (Zuur et al., 2007), the authors now provide an expanded introduction to using regression and its extensions in analysing ecological data. As with the earlier book, real data sets from postgraduate ecological studies or research projects are used throughout. The first part of the book is a largely non-mathematical introduction to linear mixed effects modelling, GLM and GAM, zero inflated models, GEE, GLMM and GAMM. The second part provides ten case studies that range from koalas to deep sea research. These chapters provide an invaluable insight into analysing complex ecological datasets, including comparisons of different approaches to the same problem. By matching ecological questions and data structure to a case study, these chapters provide an excellent starting point to analysing your own data. Data and R code from all chapters are available from www.highstat.com.\n Zuur \u0026amp; Ieno (2016):\n Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis ofmultifaceted interrelated datamake obtaining more accu- rate andmeaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming. We offer a 10-step protocol to streamline analysis of data thatwill enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending themodel via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature. Following this protocol will reduce the organization, analysis and presentation ofwhatmay be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.  Gelman \u0026amp; Hill (2007): Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors\u0026rsquo; own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.\n Lindenmayer \u0026amp; Likens (2010): Long-term monitoring programs are fundamental to understanding the natural environment and effectively tackling major environmental problems. Yet they are often done very poorly and ineffectively. Effective Ecological Monitoring describes what makes successful and unsuccessful long-term monitoring programs. Short and to the point, it illustrates key aspects with case studies and examples. It is based on the collective experience of running long-term research and monitoring programs of the two authors \u0026ndash; experience which spans more than 70 years. The book first outlines why long-term monitoring is important, then discusses why long-term monitoring programs often fail. The authors then highlight what makes good and effective monitoring. These good and bad aspects of long-term monitoring programs are further illustrated in the fourth chapter of the book. The final chapter sums up the future of long-term monitoring programs and how to make them better, more effective and better targeted.\n Bolker (2008): Ecological Models and Data in R is the first truly practical introduction to modern statistical methods for ecology. In step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and Bayesian techniques to analyze their own data using the programming language R. Drawing on extensive experience teaching these techniques to graduate students in ecology, Benjamin Bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. The book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. It requires no programming background\u0026ndash;only basic calculus and statistics.\n Practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language R Step-by-step instructions for fitting models to messy, real-world data Balanced view of different statistical approaches Wide coverage of techniques \u0026ndash; from simple (distribution fitting) to complex (state-space modeling) Techniques for data manipulation and graphical display Companion Web site with data and R code for all examples   Bibliography Agresti A. (2002). Categorical Data Analysis (Second Edition). John Wiley \u0026amp; Sons, Inc.\nBaddeley A., Rubak E. \u0026amp; Turner R. (2015). Spatial Point Patterns: Methodology and Applications with R. Chapman; Hall/CRC, Boca Raton.\nBolker B.M. (2008). Ecological Models and Data in R. Princeton University Press, Princeton, NJ.\nEmden H. van (2008). Statistics for Terrified Biologists. Blackwell Publishing.\nGelman A. \u0026amp; Hill J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, Cambridge. URL: http://www.loc.gov/catdir/enhancements/fy0668/2006040566-t.html.\nGrolemund G. \u0026amp; Wickham H. (2016). R for Data Science. URL: http://r4ds.had.co.nz/.\nHobbs N.T. \u0026amp; Hooten M.B. (2015). Bayesian Models: A Statistical Primer for Ecologists. Princeton University Press.\nJames G., Witten D., Hastie T. \u0026amp; Tibshirani R. (2013). An Introduction to Statistical Learning with Applications in R. Springer.\nKass R.E., Caffo B.S., Davidian M., Meng X.-L., Yu B. \u0026amp; Reid N. (2016). Ten Simple Rules for Effective Statistical Practice. PLOS Computational Biology 12 (6): e1004961. URL: http://dx.plos.org/10.1371/journal.pcbi.1004961. DOI: 10.1371/journal.pcbi.1004961.\nKelleher C. \u0026amp; Wagener T. (2011). Ten guidelines for effective data visualization in scientific publications. Environmental Modelling \u0026amp; Software 26 (6): 822–827. URL: https://www.sciencedirect.com/science/article/pii/S1364815210003270. DOI: 10.1016/J.ENVSOFT.2010.12.006.\nLindenmayer D. \u0026amp; Likens G.E. (2010). Effective ecological monitoring. Earthscan, London, UK.\nLohr S.L. (2010). Sampling: Design and Analysis, Second Edi. ed. Brooks/Cole.\nMcElreath R. (2015). Statistical rethinking : a Bayesian course with examples in R and Stan. Chapman; Hall/CRC, Boca Raton.\nQuinn G. \u0026amp; Keough M. (2002). Experimental design and data analysis for biologists. Cambridge University Press. URL: http://www.cambridge.org.\nvan Belle G. (2008). Statistical Rules of Thumb: Second Edition. John Wiley \u0026amp; Sons, Inc. DOI: 10.1002\u0026frasl;9780470377963.\nZuur A.F. \u0026amp; Ieno E.N. (2016). A protocol for conducting and presenting results of regression-type analyses. Methods in Ecology and Evolution 7 (6): 636–645. URL: http://doi.wiley.com/10.1111/2041-210X.12577. DOI: 10.1111\u0026frasl;2041-210X.12577.\nZuur A.F., Ieno E.N. \u0026amp; Elphick C.S. (2010). A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution 1 (9999): 3–14.\nZuur A.F., Ieno E.N. \u0026amp; Smith G.M. (2007). Analysing ecological data. Springer Verlag.\nZuur A.F., Ieno E.N., Anatoly, A \u0026amp; Saveliev (2017). Beginner’s guide to spatial, temporal, and spatial-temporal ecological data analysis with R-INLA. Highland Statistics Ltd. URL: http://www.highstat.com/Books/BGS/SpatialTemp/Zuuretal2017_TOCOnline.pdf.\nZuur A.F., Ieno E.N., Walker N.J., Saveliev A.A. \u0026amp; Smith G.M. (2009). Mixed effects models and extensions in ecology with R. Springer.\n","href":"/tutorials/articles/statistics/","title":"Books and articles on statistics"},{"content":" This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site\u0026rsquo;s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nOverviews  Wilson et al. (2017): set of good computing practices that every researcher can adopt British Ecological Society (2014): planning the data life cycle; creating, processing, documenting, preserving, sharing \u0026amp; reusing data Cooper \u0026amp; Hsing (2017): file organisation, workflow documentation, code reproducibility and readability, writing reproducible reports, version control and code archiving Ibanez et al. (2014): vision on reproducible science, routine practices, collaboration, literate computing  See also some resources related to learning and education and the discipline of open and reproducible science.\nSpecific tools  Ross et al. (2017): about tidyverse workflow and tools  Focus on version control workflows  Bryan (2017): rationale, workflows and tools regarding version control for project organization  Bibliography British Ecological Society (Ed.) (2014). A guide to data management in ecology and evolution. BES Guides to Better Science. British Ecological Society, London.\nBryan J. (2017). Excuse me, do you have a moment to talk about version control? PeerJ Preprints 5: e3159v2. URL: https://peerj.com/preprints/3159. DOI: 10.7287/peerj.preprints.3159v2.\nCooper N. \u0026amp; Hsing P.-Y. (Eds.) (2017). A guide to reproducible code in ecology and evolution. BES Guides to Better Science. British Ecological Society, London.\nIbanez L., Schroeder W.J. \u0026amp; Hanwell M.D. (2014). Practicing open science. In: Stodden V., Leisch F., Peng R.D. (editors). Implementing reproducible research. CRC Press, Boca Raton, FL.\nRoss Z., Wickham H. \u0026amp; Robinson D. (2017). Declutter your R workflow with tidy tools. PeerJ Preprints 5: e3180v1. URL: https://peerj.com/preprints/3180. DOI: 10.7287/peerj.preprints.3180v1.\nWilson G., Bryan J., Cranston K., Kitzes J., Nederbragt L. \u0026amp; Teal T.K. (2017). Good enough practices in scientific computing. PLOS Computational Biology 13 (6): e1005510. URL: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510. DOI: 10.1371/journal.pcbi.1005510.\n","href":"/tutorials/articles/computing/","title":"Scientific computing and data handling workflows"},{"content":" This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site\u0026rsquo;s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nThese resources focus on the learning and teaching aspect, while they often also give an overview of scientific computing workflows.\nOpen data science in general  Lowndes et al. (2017): learning open data science tools Hampton et al. (2017): training approaches and needed skills in data science Stevens et al. (2018): local community of practice for scientific programming: why, how (including scheme), challenges  In relation to statistics teaching  Kaplan (2017): ten organizing blocks for introductory statistics teaching in the present data science context Cetinkaya-Rundel \u0026amp; Rundel (2017): computational infrastructure and toolkit choices to allow for the necessary pedagogical innovations in statistics education  Bibliography Cetinkaya-Rundel M. \u0026amp; Rundel C.W. (2017). Infrastructure and tools for teaching computing throughout the statistical curriculum. PeerJ Preprints 5: e3181v1. URL: https://peerj.com/preprints/3181. DOI: 10.7287/peerj.preprints.3181v1.\nHampton S.E., Jones M.B., Wasser L.A., Schildhauer M.P., Supp S.R., Brun J., Hernandez R.R., Boettiger C., Collins S.L., Gross L.J., et al. (2017). Skills and Knowledge for Data-Intensive Environmental Research. BioScience 67 (6): 546–557. URL: https://academic.oup.com/bioscience/article/67/6/546/3784601. DOI: 10.1093/biosci/bix025.\nKaplan D.T. (2017). Teaching stats for data science. PeerJ Preprints 5: e3205v1. URL: https://peerj.com/preprints/3205. DOI: 10.7287/peerj.preprints.3205v1.\nLowndes J.S.S., Best B.D., Scarborough C., Afflerbach J.C., Frazier M.R., O’Hara C.C., Jiang N. \u0026amp; Halpern B.S. (2017). Our path to better science in less time using open data science tools. Nature Ecology \u0026amp; Evolution 1 (6): s41559–017–0160–017. URL: https://www.nature.com/articles/s41559-017-0160. DOI: 10.1038/s41559-017-0160.\nStevens S.L.R., Kuzak M., Martinez C., Moser A., Bleeker P.M. \u0026amp; Galland M. (2018). Building a local community of practice in scientific programming for Life Scientists. bioRxiv 265421. URL: https://www.biorxiv.org/content/early/2018/02/15/265421. DOI: 10.1101\u0026frasl;265421.\n","href":"/tutorials/articles/skills/","title":"Acquiring the skills"},{"content":" This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site\u0026rsquo;s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nSharing data  Ellis \u0026amp; Leek (2017): guidelines for providing data to a scientist / statistician: provide raw data, format consistently, include metadata \u0026amp; preprocessing steps Wilkinson et al. (2016): the FAIR data principles: date are to be findable, accessible, interoperable and reusable Culina et al. (2018): overview of online data infrastructures and considerations to be made Perkel (2016): data repository sites like github and others  Communicating  Smith et al. (2016): recommendations of the FORCE11 Software Citation Working Group  Bibliography Culina A., Baglioni M., Crowther T.W., Visser M.E., Woutersen-Windhouwer S. \u0026amp; Manghi P. (2018). Navigating the unfolding open data landscape in ecology and evolution. Nature Ecology \u0026amp; Evolution 2 (3): 420–426. URL: https://www.nature.com/articles/s41559-017-0458-2. DOI: 10.1038/s41559-017-0458-2.\nEllis S.E. \u0026amp; Leek J.T. (2017). How to share data for collaboration. PeerJ Preprints 5: e3139v5. URL: https://peerj.com/preprints/3139. DOI: 10.7287/peerj.preprints.3139v5.\nPerkel J. (2016). Democratic databases: Science on GitHub. Nature News 538 (7623): 127. URL: http://www.nature.com/news/democratic-databases-science-on-github-1.20719. DOI: 10.1038/538127a.\nSmith A.M., Katz D.S. \u0026amp; Niemeyer K.E. (2016). Software citation principles. PeerJ Computer Science 2: e86. URL: https://peerj.com/articles/cs-86. DOI: 10.7717/peerj-cs.86.\nWilkinson M.D., Dumontier M., Aalbersberg I.J., Appleton G., Axton M., Baak A., Blomberg N., Boiten J.-W., Silva Santos L.B. da, Bourne P.E., et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3: 160018. URL: http://dx.doi.org/10.1038/sdata.2016.18.\n","href":"/tutorials/articles/communicating/","title":"Sharing and communicating"},{"content":" This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site\u0026rsquo;s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nThese resources focus on the discipline as a whole, its perception, principles ,etc., while they often also give an overview of scientific computing workflows.\n Ibanez et al. (2014): open and reproducible science: vision, routine practices, collaboration, literate computing Hampton et al. (2015): workflows, tools, obstacles and needed mindshifts for open science Donati \u0026amp; Woolston (2017): how data science is becoming a large discipline  Focus on reproducible research  Stodden et al. (2014): book on computational reproducibility and (experiment) replicability; the three parts are Tools, Practices and Guidelines, Platforms Stodden \u0026amp; Miguez (2014): a formalized set of best practice recommendations for reproducible research Begley et al. (2015): current irreproducibility and good institutional practice  Bibliography Begley C.G., Buchan A.M. \u0026amp; Dirnagl U. (2015). Institutions must do their part for reproducibility. Nature 525 (7567): 25–27. URL: http://www.nature.com/news/robust-research-institutions-must-do-their-part-for-reproducibility-1.18259. DOI: 10.1038/525025a.\nDonati G. \u0026amp; Woolston C. (2017). Information management: Data domination. Nature 548 (7669): 613–614. URL: https://www.nature.com/nature/journal/v548/n7669/full/nj7669-613a.html?foxtrotcallback=true. DOI: 10.1038/nj7669-613a.\nHampton S.E., Anderson S.S., Bagby S.C., Gries C., Han X., Hart E.M., Jones M.B., Lenhardt W.C., MacDonald A., Michener W.K., et al. (2015). The Tao of open science for ecology. Ecosphere 6 (7): 1–13. URL: http://onlinelibrary.wiley.com/doi/10.1890/ES14-00402.1/abstract. DOI: 10.1890/ES14-00402.1.\nIbanez L., Schroeder W.J. \u0026amp; Hanwell M.D. (2014). Practicing open science. In: Stodden V., Leisch F., Peng R.D. (editors). Implementing reproducible research. CRC Press, Boca Raton, FL.\nStodden V. \u0026amp; Miguez S. (2014). Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research. Journal of Open Research Software 2 (1). URL: http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/. DOI: 10.5334/jors.ay.\nStodden V., Leisch F. \u0026amp; Peng R.D. (2014). Implementing reproducible research. CRC Press, Boca Raton, FL.\n","href":"/tutorials/articles/open_science/","title":"The discipline of open science"},{"content":" Introduction R code can become elaborate and consequently unclear or difficult to navigate. Yet, it is possible to introduce headers and navigate through them.\nCreating sections manually To create a header of a section, different methods can be applied. Any comment line which includes at least four trailing dashes (-), equal signs (=), or hash tags (#) automatically creates a code section.\n# 1. Header 1 #### # 2. Header 2 ---- # 3. Header 3 ==== On the right side of the code editor, nex to the buttons to run your code, a button with horizontal lines can be found. When you click it, the headers will be visible. As such, the structure of your code is visible and allows more easily to navigate through it.\nAnother way of navigation is via the button with the name of the selected header On the bottom of the code editor.\nCreating sections automatically It is also possible to add sections automatically by clicking on the tab Code and select Insert Section\u0026hellip;\nDrop down Note there is a drop down button next to each header, allowing to collapse or expand your code. Yet, there are shortcuts to this:\n Collapse — Alt+L Expand — Shift+Alt+L Collapse All — Alt+O Expand All — Shift+Alt+O  An example Now we will illustrate its use with an example of an analysis.\nRun tidyverse package\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0.9000 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.6 ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()  Now different manipulations will be performed on the dataset. To make navigation through the different manipulations more straightforward, we add sections.\n# 1. Plot hindfoot length over weight per year ---- surveys \u0026lt;- read_csv(\u0026#34;../data/20180222_surveys.csv\u0026#34;) %\u0026gt;% filter(!is.na(weight), # remove missing weight !is.na(hindfoot_length), # remove missing hindfoot_length !is.na(sex)) # remove missing sex ggplot(surveys, aes(x = weight, y = hindfoot_length)) + geom_point(aes(colour = species_id), alpha = 0.5) + ylab(\u0026#34;hindfoot length\u0026#34;) + scale_x_log10() + scale_color_discrete() + theme_dark() + facet_wrap(~year) # 2. Create a heatmap of the population growth in Ghent and its districts ---- tidy_bevolking \u0026lt;- read_csv(\u0026#34;../data/20180522_gent_groeiperwijk_tidy.csv\u0026#34;) ggplot(tidy_bevolking, aes(x = year, y = wijk)) + geom_tile(aes(fill = growth), color = \u0026#34;red\u0026#34;) + # fill = colour of content/pane; color = colour of edge # scale_fill_gradient(low = \u0026#34;white\u0026#34;, high = \u0026#34;steelblue\u0026#34;) + scale_fill_distiller(type = \u0026#34;div\u0026#34;) + theme(axis.title.x=element_blank(), axis.title.y=element_blank()) # 3. Place two plots in one window ---- install.packages(\u0026#34;cowplot\u0026#34;) devtools::install_github(\u0026#34;inbo/INBOtheme\u0026#34;) # install inbo theme library(cowplot) library(INBOtheme) weight_scatter \u0026lt;- ggplot(surveys, aes(x = weight, y = hindfoot_length)) + geom_point() + ylab(\u0026#34;hindfoot length\u0026#34;) weight_density \u0026lt;- ggplot(surveys, aes(x = weight, y = ..density..) ) + # the \u0026#39;..\u0026#39; refers to internal calculations of the density geom_histogram() + geom_density() # two plots in one window plot_grid(weight_scatter, weight_density, labels = c(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;))","href":"/tutorials/tutorials/r_script_sections/","title":"Headers and navigation in R code"},{"content":" Introduction The required packages are leaflet and sp.\nlibrary(leaflet) library(sp) Dummy data Let\u0026rsquo;s create a dumy data.frame to play around, i.e. the three locations of INBO:\nnames \u0026lt;- c(\u0026#34;VAC HT\u0026#34;,\u0026#34;Geraardsbergen\u0026#34;,\u0026#34;Linkebeek\u0026#34;) lat \u0026lt;- c(50.865664, 50.760201, 50.767950) lon \u0026lt;- c(4.349944, 3.874300, 4.333044) data \u0026lt;- data.frame(names,lat,lon) We created three points:\nplot(data$lon, data$lat) Creating a map We need to convert the data.frame to a SpatialPointsDataFrame:\ncrs_wgs84 \u0026lt;- CRS(\u0026#34;+init=epsg:4326\u0026#34;) pts \u0026lt;- SpatialPointsDataFrame(data[c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;)], data[!(names(data) %in% c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;))], proj4string = crs_wgs84) The leaflet package is ideal to create a basic interactive map:\nlibrary(leaflet) leaf_map \u0026lt;- leaflet(pts) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addCircleMarkers()leaf_map Nice, no?!\nMore information is provided at the leaflet information website!\n","href":"/tutorials/tutorials/spatial_create_leaflet_map/","title":"Let's create an interactive map!"},{"content":" Introduction This tutorial will explain how you can match a list of \u0026lsquo;scientific names\u0026rsquo; to the GBIF taxonomic backbone\nImportant is that you have rgbif and inborutils installed and available:\nlibrary(tidyverse) # tidyverse library(rgbif) # To Match GBIF library(inborutils) # wrap GBIF api data library(knitr) Read data file containing the scientific names Read file containing the scientific names you want to check against the GBIF taxonomic backbone:\nspecies_list \u0026lt;- read_csv(\u0026#34;https://raw.githubusercontent.com/inbo/inbo-pyutils/master/gbif/gbif_name_match/sample.csv\u0026#34;, trim_ws = TRUE, col_types = cols()) Take a look at the data:\nknitr::kable(head(species_list))    name kingdom euConcernStatus     Alopochen aegyptiaca Animalia under consideration   Cotoneaster ganghobaensis Plantae NA   Cotoneaster hylmoei Plantae NA   Cotoneaster x suecicus Plantae NA   Euthamia graminifolia Plantae under preparation    Request species information Match the column containing the scientificName with GBIF, using the gbif_species_name_match function from the inborutils package:\nspecies_list_matched \u0026lt;- species_list %\u0026gt;% gbif_species_name_match(name_col = \u0026#34;name\u0026#34;)  ## [1] \u0026quot;All column names present\u0026quot;  The name_col argument is the column name containing the scientific names.\nTake a look at the updated data:\nknitr::kable(head(species_list_matched))    name kingdom euConcernStatus usageKey scientificName rank order matchType phylum genus class confidence synonym status family     Alopochen aegyptiaca Animalia under consideration 2498252 Alopochen aegyptiaca (Linnaeus, 1766) SPECIES Anseriformes EXACT Chordata Alopochen Aves 98 FALSE ACCEPTED Anatidae   Cotoneaster ganghobaensis Plantae NA 3025989 Cotoneaster ganghobaensis J.Fryer \u0026amp; B.Hylmö SPECIES Rosales EXACT Tracheophyta Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster hylmoei Plantae NA 3025758 Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer SPECIES Rosales EXACT Tracheophyta Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster x suecicus Plantae NA 3026040 Cotoneaster suecicus G.Klotz SPECIES Rosales EXACT Tracheophyta Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Euthamia graminifolia Plantae under preparation 3092782 Euthamia graminifolia (L.) Nutt. SPECIES Asterales EXACT Tracheophyta Euthamia Magnoliopsida 98 FALSE ACCEPTED Asteraceae    You can also specify which info (gbif_terms) GBIF should return. For example, return a lot of terms:\nspecies_list %\u0026gt;% gbif_species_name_match(name_col = \u0026#34;name\u0026#34;, gbif_terms = c(\u0026#39;usageKey\u0026#39;, \u0026#39;scientificName\u0026#39;, \u0026#39;rank\u0026#39;, \u0026#39;order\u0026#39;, \u0026#39;matchType\u0026#39;, \u0026#39;phylum\u0026#39;, \u0026#39;kingdom\u0026#39;, \u0026#39;genus\u0026#39;, \u0026#39;class\u0026#39;, \u0026#39;confidence\u0026#39;, \u0026#39;synonym\u0026#39;, \u0026#39;status\u0026#39;, \u0026#39;family\u0026#39;)) %\u0026gt;% head() %\u0026gt;% kable() ## [1] \u0026quot;All column names present\u0026quot;     name kingdom euConcernStatus usageKey scientificName rank order matchType phylum genus class confidence synonym status family     Alopochen aegyptiaca Animalia under consideration 2498252 Alopochen aegyptiaca (Linnaeus, 1766) SPECIES Anseriformes EXACT Chordata Alopochen Aves 98 FALSE ACCEPTED Anatidae   Cotoneaster ganghobaensis Plantae NA 3025989 Cotoneaster ganghobaensis J.Fryer \u0026amp; B.Hylmö SPECIES Rosales EXACT Tracheophyta Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster hylmoei Plantae NA 3025758 Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer SPECIES Rosales EXACT Tracheophyta Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster x suecicus Plantae NA 3026040 Cotoneaster suecicus G.Klotz SPECIES Rosales EXACT Tracheophyta Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Euthamia graminifolia Plantae under preparation 3092782 Euthamia graminifolia (L.) Nutt. SPECIES Asterales EXACT Tracheophyta Euthamia Magnoliopsida 98 FALSE ACCEPTED Asteraceae    or rather just a subset:\nspecies_list %\u0026gt;% gbif_species_name_match(name_col = \u0026#34;name\u0026#34;, gbif_terms = c(\u0026#39;scientificName\u0026#39;, \u0026#39;family\u0026#39;, \u0026#39;order\u0026#39;, \u0026#39;matchType\u0026#39;)) %\u0026gt;% head() %\u0026gt;% kable() ## [1] \u0026quot;All column names present\u0026quot;     name kingdom euConcernStatus scientificName family order matchType     Alopochen aegyptiaca Animalia under consideration Alopochen aegyptiaca (Linnaeus, 1766) Anatidae Anseriformes EXACT   Cotoneaster ganghobaensis Plantae NA Cotoneaster ganghobaensis J.Fryer \u0026amp; B.Hylmö Rosaceae Rosales EXACT   Cotoneaster hylmoei Plantae NA Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer Rosaceae Rosales EXACT   Cotoneaster x suecicus Plantae NA Cotoneaster suecicus G.Klotz Rosaceae Rosales EXACT   Euthamia graminifolia Plantae under preparation Euthamia graminifolia (L.) Nutt. Asteraceae Asterales EXACT    ","href":"/tutorials/tutorials/r_gbif_name_matching/","title":"Match scientific names with the GBIF backbone"},{"content":" library(dplyr) How to use piping in R Normally, you would do this:\nhead(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  However, with piping, this would look different:\nmtcars %\u0026gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  You may wonder, what\u0026rsquo;s the point? If you need to apply multiple functions on one dataframe, piping saves you a lot of typing, and makes for tidy R code. An example:\nmtcars %\u0026gt;% mutate(dec = mpg/10) %\u0026gt;% select(mpg, dec, am) %\u0026gt;% filter(am == \u0026#34;1\u0026#34;) ## mpg dec am ## 1 21.0 2.10 1 ## 2 21.0 2.10 1 ## 3 22.8 2.28 1 ## 4 32.4 3.24 1 ## 5 30.4 3.04 1 ## 6 33.9 3.39 1 ## 7 27.3 2.73 1 ## 8 26.0 2.60 1 ## 9 30.4 3.04 1 ## 10 15.8 1.58 1 ## 11 19.7 1.97 1 ## 12 15.0 1.50 1 ## 13 21.4 2.14 1  What did we do: 1. We created a new column \u0026lsquo;dec\u0026rsquo; using mutate(). This column \u0026lsquo;dec\u0026rsquo; consists of the values of column mpg divided by 10. 2. We selected the columns \u0026lsquo;mpg\u0026rsquo;, \u0026lsquo;dec\u0026rsquo; and \u0026lsquo;am\u0026rsquo; using select(). 3. We filtered for the value \u0026lsquo;1\u0026rsquo; in the column \u0026lsquo;am\u0026rsquo; using filter().\nAnd all of this in just one step! Now what? We have created a new column, but this column is not part of our dataframe yet! We could do this:\nmtcars \u0026lt;- mtcars %\u0026gt;% mutate(dec = mpg/10) OR\u0026hellip; we could do this!\nlibrary(magrittr) mtcars %\u0026lt;\u0026gt;% mutate(dec = mpg/10) Soooo easy! This has been our first introduction to piping. There is however much more to learn! That is why you should definitely go to this link.\n","href":"/tutorials/tutorials/r_tidyverse_piping/","title":"Using `%\u003e%` pipes in R"},{"content":" Insync is a thirth party tool that synchronises files with Google Drive. It has some nice features which are still not available in the sync tools provided by Google. For the remaining of this tutorial, \u0026ldquo;GoogleDrive\u0026rdquo; refers to the sync tools provided by Google.\nThe problem with GoogleDrive GoogleDrive doesn\u0026rsquo;t work well in combination with RStudio projects or Git projects. We\u0026rsquo;ll illustrate the problem with RStudio. RStudio has a performant auto save functionality, which limits data loss after an unexpected crash. As soon as the user changes a few characters in a script, the auto save kicks in. This functionality stores the backup information into a hidden subdirectory of the project (.Rproj.user). It writes very often to the files in this subdirectory.\nGoogleDrive is constantly monitoring the synchronised directory for new, changed or deleted files. As soon as it detects such file, it will lock the file, synchronise the file and unlock the file. The locking of the file pervents that changes are made to the file while it is being synchronised, because this would mess up the synchronisation. GoogleDrive synchronises all files within a synced directory, including those created by the RStudio auto save function. But as this function writes very often to those files, it often ends in trying to write to a file which still is locked by GoogleDrive. This results in a \u0026ldquo;cannot save to file\u0026rdquo; dialog box in RStudio, which has to be dismissed by the user. This happens so often that it becomes frustrating for the user.\nHow Insync solves this problem Insync is also constantly monitoring all files in the synchronised directories. Unlike GoogleDrive, Insync has an option to ignore directories or files when synchronising. When Insync is set to ignore .Rproj.user, the files within .Rproj.user are no longer synchronised and thus never locked, causing no problem with the RStudio auto save function.\nWait a minute, so these files will be no longer be available through the GoogleDrive website? Isn\u0026rsquo;t that a problem? Indeed, they will not be available. And no, that is not a problem. Only your temporary changes are no longer synchronised. When you save your script file in RStudio, you are saving a file to a location which is not on the ignore list and will thus be synchronised. But after saving, this file will be locked during sync? Yes, but the time between two consecutive manual saves of a script is a lot larger that the time required to sync the script. So the file will be unlocked by the next time you save the file. How to set the ignore list in Insync First of all, it is important to do this prior to syncing files to your computer. Once a file or directory has been synced between the computer and the cloud, Insync will keep syncing it. Even when a file or directory is afterward added to the ignore list.\nSet up  Open the Insync app Click on your avatar Choose ignore list Add the search pattern for the files/folders to ignore into the form fieldand click on the circled \u0026lsquo;+\u0026rsquo;  The default action is to exclude all matching files and directories (including their files and subdirectories) from syncing (\u0026ldquo;do not upload or download\u0026rdquo;). Local files will remain only local and files in the cloud will remain only in the cloud. You can change this behaviour via the drop down menu of the pattern. Other options are \u0026ldquo;do not upload\u0026rdquo;, \u0026ldquo;do not download\u0026rdquo; or \u0026ldquo;remove from this list\u0026rdquo;.\nWe recommend to add following patterns:\n .rproj.user *.git *.rcheck *_cache *_files _site   FAQ  Can I use the same local folder when switching from GoogleDrive to Insync  It is safer you use a different folder.  I\u0026rsquo;ve already synced an RStudio project with Insync without setting the ignore list  Create a new RStudio project in a different folder and copy your data an script to this new RStudio project   ","href":"/tutorials/installation/user/user_install_insync/","title":"Insync installation"},{"content":" Set up continuous integration with Wercker There are 2 major steps to set up continuous integration:\n create a wercker.yml file in the package add the application (package) to Wercker.com  To be able to add a package to Wercker, one must have administrator rights on the package repository on Github.\nThe Wercker test environment can only be set up if the file wercker.yml is commited to the repository, but Wercker is triggered to start checking when the application is added to wercker.com (giving an error if wercker.yml is not commited yet).\nwercker.yml Add a file \u0026ldquo;wercker.yml\u0026rdquo; in the root of the package with:\n box: reference to a package with a Docker image that is used as a test environment. If no specific version is specified, only the last master version is used. Which Docker image to use?  inbobmk/rstable which is an image with stable versions of R and a large number of packages (see the README). Most of the packages which are often used at INBO are available. The version of the packages is roughly fixed to the date on which the R version in the Docker image was upgraded. rocker/verse (https://hub.docker.com/r/rocker/verse/) which has the R, devtools and all tidyverse packages. The latest version of the image contains the latest version of the packages.  build:  different steps to pass  inbobmk/r-check: runs R CMD check but assumes that all dependencies are installed. Use this in combination with inbobmk/rstable in case you want to check your package against a stable set of packages. jimhester/r-check: installs all missing dependencies on the fly and then runs R CMD check. This will install the latest version of the dependencies. inbobmk/r-coverage: checks which lines in the code are covered by unit tests and which are not. See our page on code coverage for more details. This assumes that the covr package is installed. jimhester/r-coverage: installs covr and runs the code coverage inbobmk/r-lint: this check the style of your code. Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. It assumes the lintr package is installed. jimhester/r-lint: installs the lintr package and checks the style of the code  steps are run following their order in the yaml file the execution will stop when a step fails if not all packages are available in the docker image, code to install packages has to be added as a first step  after a build pipe, one can also add a deploy-pipe  An example of a simple wercker.yml-file:\nbox: inbobmk/rstable build: steps: - script: code: Rscript -e \u0026quot;install.packages(c('DT','plotly'))\u0026quot; - inbobmk/r-check - inbobmk/r-coverage - inbobmk/r-lint  Wercker.com To add the application to www.wercker.com:\n log in on the website (easiest is to log in via github) and create a username click on the \u0026ldquo;+\u0026rdquo; button on the right top and choose \u0026ldquo;add application\u0026rdquo; select your username (next) select the repository of your package (next) choose the recommended option (next) you could choose to make the results publicly available and \u0026ldquo;create\u0026rdquo;  After creation, one can under \u0026ldquo;options\u0026rdquo;:\n pick a color for the package (useful when adding more than one package) read information on Webhook (ensures communication between github, Wercker and other services) status badge: markdown-code that allows you to add Wercker-results to your own code (e.g. copy this link to the README-file)  Hitting the avatar on the top right and choosing \u0026ldquo;Settings\u0026rdquo; allows to adjust if and when to receive email notifications.\n","href":"/tutorials/tutorials/development_wercker/","title":"Wercker"},{"content":" What is WFS? In computing, the Open Geospatial Consortium Web Feature Service (WFS) Interface Standard provides an interface allowing requests for geographical features across the web using platform-independent calls. One can think of geographical features as the \u0026ldquo;source code\u0026rdquo; behind a map, whereas the WMS interface or online tiled mapping portals like Google Maps return only an image, which end-users cannot edit or spatially analyze. The XML-based GML furnishes the default payload-encoding for transporting geographic features, but other formats like shapefiles can also serve for transport. In early 2006 the OGC members approved the OpenGIS GML Simple Features Profile. This profile is designed both to increase interoperability between WFS servers and to improve the ease of implementation of the WFS standard. (Source: Wikipedia)\nDownload vector data from WFS This examples illustrates how you can download information from a WFS for further use in R. First of all we need to URL of the service. Note that we appended the protocol WFS: in front of the URL. ogrinfo() from gdalUtils is used to extra some information on the service. cat() prints this information. The summary indicates that the service provides three layers: \u0026ldquo;BWK:Bwkhab\u0026rdquo;, \u0026ldquo;BWK:Bwkfauna\u0026rdquo; and \u0026ldquo;BWK:Hab3260\u0026rdquo;. It also reveals the kind of layers, in this case two polygon layers and one line layer.\nwfs_bwk \u0026lt;- \u0026#34;WFS:https://geoservices.informatievlaanderen.be/overdrachtdiensten/BWK/wfs\u0026#34; wfs_weather \u0026lt;- \u0026#34;WFS:http://opendata.meteo.be/service/aws/ows\u0026#34; library(gdalUtils) info \u0026lt;- ogrinfo(wfs_bwk, so = TRUE) cat(info, sep = \u0026#34;\\n\u0026#34;) ## Had to open data source read-only. ## INFO: Open of `WFS:https://geoservices.informatievlaanderen.be/overdrachtdiensten/BWK/wfs' ## using driver `WFS' successful. ## Metadata: ## ABSTRACT=Directe downloadservice voor de Biologische Waarderingskaart en Natura 2000 Habitatkaart. ## PROVIDER_NAME=agentschap Informatie Vlaanderen ## TITLE=WFS Biologische Waarderingskaart en Natura 2000 Habitatkaart ## 1: BWK:Bwkhab (Curve Polygon) ## 2: BWK:Bwkfauna (Curve Polygon) ## 3: BWK:Hab3260 (Line String)  Let\u0026rsquo;s start by downloading the \u0026ldquo;BWK:Bwkhab\u0026rdquo; layer for the Hallerbos area. We use ogr2ogr() from the gdalUtils package. The main part is defining the input and output. We store the data in GeoJSON format which is an open standard format designed for representing simple geographical features, along with their non-spatial attributes. It is based on JSON, the JavaScript Object Notation.\nWe also add the bounding box from which we want to retrieve the data. This is very important to add. If you omit a bounding box, the service will return the entire map which can be very large.\nogr2ogr( src_datasource_name = wfs_bwk, # the input source layer = \u0026#34;BWK:Bwkhab\u0026#34;, # the layer from the input dst_datasource_name = \u0026#34;bwkhab.geojson\u0026#34;, # the target file f = \u0026#34;geojson\u0026#34;, # the target format spat = c(left = 142600, bottom = 153800, right = 146000, top = 156900), # the bounding box t_srs = \u0026#34;EPSG:31370\u0026#34;, # the coordinate reference system verbose = TRUE ) ## Checking gdal_installation... ## Scanning for GDAL installations... ## Checking the gdalUtils_gdalPath option... ## GDAL version 2.2.2 ## GDAL command being used: \u0026quot;/usr/bin/ogr2ogr\u0026quot; -spat 142600 153800 146000 156900 -f \u0026quot;geojson\u0026quot; -t_srs \u0026quot;EPSG:31370\u0026quot; \u0026quot;bwkhab.geojson\u0026quot; \u0026quot;WFS:https://geoservices.informatievlaanderen.be/overdrachtdiensten/BWK/wfs\u0026quot; \u0026quot;BWK:Bwkhab\u0026quot; ## character(0)  At this point, all features are downloaded and can be used in R as we would we any other local file. So we need to load the file with readOGR() from rgdal.\nlibrary(rgdal) ## Loading required package: sp ## rgdal: version: 1.3-6, (SVN revision 773) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.2.2, released 2017/09/15 ## Path to GDAL shared files: /usr/share/gdal/2.2 ## GDAL binary built with GEOS: TRUE ## Loaded PROJ.4 runtime: Rel. 4.9.2, 08 September 2015, [PJ_VERSION: 492] ## Path to PROJ.4 shared files: (autodetected) ## Linking to sp version: 1.2-3  bwkhab \u0026lt;- readOGR(\u0026#34;bwkhab.geojson\u0026#34;, stringsAsFactors = FALSE) ## OGR data source with driver: GeoJSON ## Source: \u0026quot;/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/spatial_wfs_services/bwkhab.geojson\u0026quot;, layer: \u0026quot;BWK:Bwkhab\u0026quot; ## with 314 features ## It has 32 fields  summary(bwkhab) ## Object of class SpatialPolygonsDataFrame ## Coordinates: ## min max ## x 137307.1 146356.1 ## y 153734.4 165353.8 ## Is projected: TRUE ## proj4string : ## [+proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 ## +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 ## +ellps=intl ## +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 ## +units=m +no_defs] ## Data attributes: ## gml_id UIDN OIDN TAG ## Length:314 Min. : 1184 Min. : 1184 Length:314 ## Class :character 1st Qu.:175124 1st Qu.:173440 Class :character ## Mode :character Median :338130 Median :335492 Mode :character ## Mean :332296 Mean :327872 ## 3rd Qu.:491850 3rd Qu.:488782 ## Max. :659382 Max. :637654 ## EVAL EENH1 EENH2 ## Length:314 Length:314 Length:314 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## EENH3 EENH4 EENH5 ## Length:314 Length:314 Length:314 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## EENH6 EENH7 EENH8 ## Length:314 Length:314 Length:314 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## V1 V2 V3 ## Length:314 Length:314 Length:314 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## HERK INFO BWKLABEL ## Length:314 Length:314 Length:314 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## HAB1 PHAB1 HAB2 PHAB2 ## Length:314 Min. : 40.00 Length:314 Min. : 0.000 ## Class :character 1st Qu.:100.00 Class :character 1st Qu.: 0.000 ## Mode :character Median :100.00 Mode :character Median : 0.000 ## Mean : 95.83 Mean : 4.108 ## 3rd Qu.:100.00 3rd Qu.: 0.000 ## Max. :100.00 Max. :50.000 ## HAB3 PHAB3 HAB4 PHAB4 ## Length:314 Min. : 0.00000 Length:314 Min. :0 ## Class :character 1st Qu.: 0.00000 Class :character 1st Qu.:0 ## Mode :character Median : 0.00000 Mode :character Median :0 ## Mean : 0.06369 Mean :0 ## 3rd Qu.: 0.00000 3rd Qu.:0 ## Max. :20.00000 Max. :0 ## HAB5 PHAB5 HERKHAB HERKPHAB ## Length:314 Min. :0 Length:314 Length:314 ## Class :character 1st Qu.:0 Class :character Class :character ## Mode :character Median :0 Mode :character Mode :character ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## HABLEGENDE ## Length:314 ## Class :character ## Mode :character ## ## ##  Let\u0026rsquo;s make a simple plot of the object. Note that the object contains features outside of the bounding box. Those are features which have only some part within the bounding box.\nspplot(bwkhab, zcol = \u0026#34;PHAB1\u0026#34;, scales = list(draw = TRUE)) Download feature attribute data from WFS In some situations, we do not need the spatial features (polygons, lines, points), but are interested in the data at a particular point (i.e. attribute table data) of the spatial feature. When working in a local GIS environment, one would use a spatial operator to extract the data (e.g. within, intersects, contains,\u0026hellip;). Actually, WFS supports certain spatial operators as part of the service to directly query this data and overcomes the need to download the spatial feature data first.\nWe rely on the httr package to talk to web services:\nlibrary(httr) # generic webservice package Consider the following use case: You want to extract the attribute data from a soil map for a number of sampling points (point coordinates). This use case can be tackled by relying on the WFS service and the affiliated spatial operators.\nOur example data point (in Lambert 72):\nx_lam \u0026lt;- 173995.67 y_lam \u0026lt;- 212093.44 From this point we know the data, so we can verify the result (in dutch):\n Bodemtype: s-Pgp3(v) Bodemserie: Pgp Textuurklasse: licht zandleem Drainageklasse: uiterst nat, gereduceerd  Hence, we now want to extract these soil properties from the WFS, for the coordinates defined above.\nproperties_of_interest \u0026lt;- c(\u0026#34;Drainageklasse\u0026#34;, \u0026#34;Textuurklasse\u0026#34;, \u0026#34;Bodemserie\u0026#34;, \u0026#34;Bodemtype\u0026#34;) The URL of the wfs service of the soil map of the Flemish region:\nwfs_bodemtypes \u0026lt;- \u0026#34;https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wfs?\u0026#34; The essential part is to set up the proper query! The required data for the service is defined in the metadata description. This can look a bit overwhelming in the start, but is a matter of looking for some specific elements of the (XML) document:\n service (WFS), request (GetFeature) and version (1.1.0) are mandatory fields (see below) typeName: Look at the different \u0026lt;FeatureType... enlisted and pick the \u0026lt;Name\u0026gt; of the one you\u0026rsquo;re interested in. In this particular case bodemkaart:bodemtypes is the only one available. outputFormat: The supported output formats are enlisted in \u0026lt;ows:Parameter name=\u0026quot;outputFormat\u0026quot;\u0026gt;. As the service provides CSV as output, this is a straightforward option. json is another popular one. propertyname: A list of the attribute table fields (cfr. supra). A full list of the Flanders soil map is provided here. We also define the CRS, using the EPSG code. CQL_FILTER: Define the spatial operator, in this case INTERSECTS of the WFS geom and our POINT coordinate. The operators are enlisted in the \u0026lt;fes:SpatialOperators\u0026gt; field.  Formatting all this information in a query and executing the request (GET) towards the service:\nquery = list(service = \u0026#34;WFS\u0026#34;, request = \u0026#34;GetFeature\u0026#34;, version = \u0026#34;1.1.0\u0026#34;, typeName = \u0026#34;bodemkaart:bodemtypes\u0026#34;, outputFormat = \u0026#34;csv\u0026#34;, propertyname = as.character(paste(properties_of_interest, collapse = \u0026#34;,\u0026#34;)), CRS = \u0026#34;EPSG:31370\u0026#34;, CQL_FILTER = sprintf(\u0026#34;INTERSECTS(geom,POINT(%s %s))\u0026#34;, x_lam, y_lam)) result \u0026lt;- GET(wfs_bodemtypes, query = query) result ## Response [https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wfs?service=WFS\u0026amp;request=GetFeature\u0026amp;version=1.1.0\u0026amp;typeName=bodemkaart%3Abodemtypes\u0026amp;outputFormat=csv\u0026amp;propertyname=Drainageklasse%2CTextuurklasse%2CBodemserie%2CBodemtype\u0026amp;CRS=EPSG%3A31370\u0026amp;CQL_FILTER=INTERSECTS%28geom%2CPOINT%28173995.67%20212093.44%29%29] ## Date: 2018-12-13 15:13 ## Status: 200 ## Content-Type: text/csv;charset=UTF-8 ## Size: 129 B ## FID,Bodemtype,Bodemserie,Textuurklasse,Drainageklasse ## bodemtypes.72727,s-Pgp3(v),Pgp,licht zandleem,\u0026quot;uiterst nat, gereduceerd\u0026quot;  The result is not yet formatted to be used as a dataframe. We need to use a small trick using the textConnection function to get from the result (bits) towards a readable output in a dataframe:\ndf \u0026lt;- read.csv(textConnection(content(result, \u0026#39;text\u0026#39;))) kable(df)    FID Bodemtype Bodemserie Textuurklasse Drainageklasse     bodemtypes.72727 s-Pgp3(v) Pgp licht zandleem uiterst nat, gereduceerd    Which indeed corresponds to the data of the coordinate.\n","href":"/tutorials/tutorials/spatial_wfs_services/","title":"Using WFS service in R"},{"content":"Sometimes we have a layer in one coordinate reference system (CRS) and need to transform it into another coordinate reference system. The first thing we need to do is identifying both coordinate reference systems. Let\u0026rsquo;s create an example and identify the coordinate reference system with proj4string(). We used the coordinates posted on the contact page of NGI.\nlibrary(sp) library(leaflet) library(widgetframe) ## Loading required package: htmlwidgets  ngi \u0026lt;- data.frame(x = 650381.78, y = 667603.12) coordinates(ngi) \u0026lt;- ~x + y proj4string(ngi) ## [1] NA  NA indicates that the coordinate reference system isn\u0026rsquo;t set. So we need to set it manually. In this case we know it is \u0026ldquo;Lambert 2008\u0026rdquo;. We need to know the related projection string. The projection string is often a rather long string of parameters. However, most coordinate reference systems have an EPSG number which you can find at http://epsg.io/. The EPSG number for \u0026ldquo;Lambert 2008\u0026rdquo; is 3812. Let\u0026rsquo;s set this coordinate reference system to our dataset. CRS() defines the coordinate reference system based on a text string.\nproj4string(ngi) \u0026lt;- CRS(\u0026#34;+init=epsg:3812\u0026#34;) proj4string(ngi) ## [1] \u0026quot;+init=epsg:3812 +proj=lcc +lat_1=49.83333333333334 +lat_2=51.16666666666666 +lat_0=50.797815 +lon_0=4.359215833333333 +x_0=649328 +y_0=665262 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\u0026quot;  We could verify the correctness of this position by plotting in on a map. Here we use the leaflet package which requires the data to be in the \u0026ldquo;WGS84\u0026rdquo; coordinate reference system. Therefore we use spTransform to do this transformation. \u0026ldquo;WGS84\u0026rdquo; has EPSG number 4326. But here the coordinate reference system string itself is easier to memorise: \u0026ldquo;+proj=longlat\u0026rdquo;.\nngi_ll \u0026lt;- spTransform(ngi, CRS(\u0026#34;+proj=longlat\u0026#34;)) proj4string(ngi_ll) ## [1] \u0026quot;+proj=longlat +ellps=WGS84\u0026quot;  leaflet(ngi_ll) %\u0026gt;% addTiles() %\u0026gt;% addMarkers() # %\u0026gt;% #frameWidget()    CRS EPSG shortened_PROJ.4_string     WGS 84 4326 +init=epsg:4326   Belge 1972 / Belgian Lambert 72 31370 +init=epsg:31370   ETRS89 / Belgian Lambert 2008 3812 +init=epsg:3812   WGS 84 / Pseudo-Mercator 3857 +init=epsg:3857    ","href":"/tutorials/tutorials/spatial_transform_crs/","title":"Transforming spatial objects"},{"content":" WMS stands for Web Map Service. The service provides prerendered tiles at different scales. This makes it useful to include them as background images in maps.\nwms_grb links to the WMS of the GRB-basiskaart, the Flemish cadastral map. It depicts land parcels, buildings, watercourses, roads and railroads.\nwms_ortho contains a mosaic of recent orthophotos made during the winter. The layer Ortho contains the images, the layer Vliegdagcontour detail on the time when the pictures were taken.\nwms_inbo is a WMS providing [several layers]()\nwms_hunting displays hunting grounds in Flanders\nwms_grb \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/GRB-basiskaart/wms\u0026#34; wms_ortho \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/OMWRGBMRVL/wms\u0026#34; wms_inbo \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/INBO/wms\u0026#34; wms_hunting \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/Jacht/wms\u0026#34; As background of interactive maps WMS layers can be added to a leaflet map using the addWMSTiles() function.\nlibrary(leaflet) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_grb, layers = \u0026#34;GRB_BSK\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_ortho, layers = \u0026#34;Ortho\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_inbo, layers = \u0026#34;PNVeg\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 14) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addWMSTiles( wms_hunting, layers = \u0026#34;Jachtterr\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 14) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addWMSTiles( wms_grb, layers = \u0026#34;GRB_BSK\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;GRB\u0026#34; ) %\u0026gt;% addWMSTiles( wms_hunting, layers = \u0026#34;Jachtterr\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;hunting\u0026lt;br\u0026gt;grounds\u0026#34; ) %\u0026gt;% addLayersControl( baseGroups = \u0026#34;OSM\u0026#34;, overlayGroups = c(\u0026#34;GRB\u0026#34;, \u0026#34;hunting\u0026lt;br\u0026gt;grounds\u0026#34;), options = layersControlOptions(collapsed = FALSE) ) ","href":"/tutorials/tutorials/spatial_wms_services/","title":"Using WMS service in R"},{"content":" Real life datasources seldom provide data in exactly the format you need for the analysis. Hence most of the time you need to manipulate the data after reading it into R. There are several ways to do this, each with their pros and cons. We highly recommend the tidyverse collection of packages. The command library(tidyverse) will actually load the following packages: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr and forecats.\nWhere to find good information on these packages:  official tidyverse website the R for data science book (R4DS) by Garrett Grolemund and Hadley Wickham. Note that this book is freely available online. A printed version is available at the INBO library. video tutorials:  Data wrangling with R and RStudio: a good introduction on dplyr and tidyr by Garrett Grolemund dplyr tutorial at useR!2014 by Hadley Wickham (video part 1 and part 2) tidyverse, visualization, and manipulation basics: a high-level overview of tidyverse by Garrett Grolemund  Data Transformation Cheat Sheet: a two page document which covers the most important function for dplyr  ","href":"/tutorials/tutorials/r_tidyverse_info/","title":"Data wrangling with tidyverse"},{"content":" Introduction (shamelessly taken from wikipedia)\nKerberos is a computer network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner.\nWindows 2000 and later uses Kerberos as its default authentication method. Many UNIX and UNIX-like operating systems, including FreeBSD, Apple\u0026rsquo;s Mac OS X, Red Hat Enterprise Linux, Oracle\u0026rsquo;s Solaris, IBM\u0026rsquo;s AIX and Z/OS, HP\u0026rsquo;s HP-UX and OpenVMS and others, include software for Kerberos authentication of users or services.\nHence, we can use the protocol to have an OS independent solution for authentication across different databases. In this document, the installation and configuration for linux/mac users is provided as well as an introduction to the usage of the authentication service to connect to databases. For windows users (in the domain) the authentication is provided by default.\nInstallation Libraries for authentication For debian/ubuntu users (make sure you belong to the sudo group):\nsudo apt-get install krb5-user libpam-krb5 libpam-ccreds auth-client-config sudo apt-get install openssl  These libraries will be used later on. The following section is for interaction with MS SQL databases.\nModern Linux distributions use PAM to handle the authentication tasks of applications (services) on the system (PAM stands for Pluggable Authentication Modules, see man PAM). However we do not need that here. The above installation may have led to inserting a line into PAM configuration file /etc/pam.d/common-auth. The line looks like this (note the defining part pam_krb5.so):\nauth\t[success=4 default=ignore]\tpam_krb5.so minimum_uid=1000  This line makes every application that needs authentication on the system (like sudo, screensaver unlock, update manager, \u0026hellip;) first try the Kerberos connection to authenticate. This is overkill as we don\u0026rsquo;t want to use Kerberos that way, and it can significantly slow down all other system authentications. Therefore, you should comment out the above line in /etc/pam.d/common-auth.\nMS SQL Server tools As most of the databases at INBO are SQL Server, an appropriate driver and the command line toolset is required to fully support database connections to SQL Server.\nODBC driver Download and install the Microsoft ODBC Driver for SQL Server. The installation instructions for different Linux flavours can be downloaded together with the ODBC driver. For Ubuntu 16.04 (and most distributions based on it), following instructions apply:\nsudo su apt-get install curl curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list \u0026gt; /etc/apt/sources.list.d/mssqlrelease.list exit sudo apt-get update sudo ACCEPT_EULA=Y apt-get install msodbcsql=13.1.4.0-1 sudo apt-get install unixodbc-dev  mssql-tools Install the MS SQL tools as well:\n sqlcmd: Command-line query utility. bcp: Bulk import-export utility.  The instructions for different platforms are explained here. In order to test the SQL connection later in this tutorial, add /opt/mssql-tools/bin/ to your PATH environment variable.\nYou could also decide to go for the binaries: download the debian package of mssql-tools and install with:\nsudo apt-get install libgss3 sudo dpkg -i mssql-tools_14.0.1.246-1_amd64.deb  Configure Kerberos client (again, the commands assume root privileges)\nStart with the kerberos configuration dialogue:\ndpkg-reconfigure krb5-config  Use INBO.BE as the realm (this is the realm of the kerberos servers): Make sure to use DNS to find these servers, so choose \u0026lsquo;NO\u0026rsquo; if you get the below question: Next, adapt the krb5.conf, probably available in the /etc directory. Add the following sections with configurations to the file:\n[realms] INBO.BE = { kdc = DNS_Name_DomainController1.domain.be kdc = DNS_Name_DomainController2.domain.be kdc = DNS_Name_DomainController3.domain.be kdc = DNS_Name_DomainController4.domain.be kdc = DNS_Name_DomainController5.domain.be default_domain = domain.be } [logging] default = FILE:/var/log/krblibs.log kdc = FILE:/var/log/krbkdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] default_realm = DOMAIN.BE dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable= true  Inbo staff can download a preconfigured krb5.conf file here:\u0026ldquo;https://drive.google.com/a/inbo.be/file/d/1q4MOWl3i-DDy1s3vwOeqPkpToa1S-3zE/view?usp=sharing\u0026quot;. In order to sync the timing of the domain controller server and client side, install ntp:\nsudo apt-get install ntp  After installation, check if the following two files do exist: * /etc/ntp.conf * /etc/ntp.conf.dhcp (empty file, just amke sure there is a file)\nTest installation Kerberos ticket system To check if the Kerberos configuration is successful, ask for a ticket by initiating with kinit:\nkinit your_user_name  If no errors are prodused, check the existing tickets with klist:\nklist  This should produce a list of successfully granted tickets, so something similar as:\nValid starting Expires Service principal 03/01/18 15:42:08 04/01/18 01:42:08 krbtgt/INBO.BE@INBO.BE renew until 10/01/18 15:42:08  SQL database connections When the ticketing is working, the next step is to use the authentication to connect to the databases itself. To test this, we\u0026rsquo;ll use the sqlcmd command line tool. In a next section, we\u0026rsquo;ll focus on the ODBC settings.\nTesting with sqlcmd (make sure you have an active ticket). Type quit to exit.\nInbo staff can consult a list of connection strings ( including server names ) for a server to query link\nsqlcmd -S DBServerName -E 1\u0026gt; Select top 10 name from sys.databases; 2\u0026gt; Go  SQL ODBC connections To support database connections from other applications (e.g. GUI environments, but also R, Python,\u0026hellip;), the configuration of database drivers and connections should be provided in the /etc/odbc.ini and /etc/odbcinst.ini.\nMake sure the ODBC driver for SQL Server is available with a recognizable name in the /etc/odbcinst.ini file:\n[ODBC Driver 13 for SQL Server] Description=Microsoft ODBC Driver 13 for SQL Server Driver=/opt/microsoft/msodbcsql/lib64/libmsodbcsql-13.1.so.4.0 UsageCount=2  Connecting by explicitly providing the SQL connection string to ODBC libraries/packages Inbo staff can consult a list of connection strings here At this moment, you can actually connect using typical ODBC libraries/packages provided by R or Python:\nlibrary(DBI) connection \u0026lt;- dbConnect( odbc::odbc(), .connection_string = \u0026#34;Driver={ODBC Driver 13 for SQL Server};Server=DBServername;Database=DBName;Trusted_Connection=yes;\u0026#34; ) dbListTables(connection)import pyodbc conn = pyodbc.connect(\u0026#34;Driver={ODBC Driver 13 for SQL Server};Server=DBServername;Database=DBName;Trusted_Connection=yes;\u0026#34;) In RStudio, you can also make the connection with the GUI:\n Go to the Connections pane and click \u0026lsquo;New Connection\u0026rsquo;. In the window that opens, choose the ODBC Driver for SQL Server. In the Parameters field that comes next, add Server=DBServerName;Database=DBName;Trusted_Connection=yes;.  Note that the DBI connection statement is visible at the bottom field of the dialog window.  Click Test to verify successful connection.  If connection is unsuccessful, try again after explicitly adding your username to the connection string: User ID=your_username;  If the test is successful, click OK to make the connection.  Beside the fact that the connection has been made (see RStudio\u0026rsquo;s R console), you also get a list of all databases (of the specific SQL Server) in the Connections pane. You can use this for exploratory purposes. Click here for more information on using RStudio\u0026rsquo;s Connections pane.\nUNTESTED: Connecting after configuring odbc.ini However, it is probably easier to provide the configuration to specific databases directly, using the /etc/odbc.ini file. For example, the DBName database can be defined as follows:\n[nbn_ipt] Driver = ODBC Driver 13 for SQL Server Description = odbc verbinding naar db Trace = No Server = DBServername Database = DBName Port = 1433  Next, add the DBServername\nTODO: -\u0026gt; example in R/Python -\u0026gt; also available in Rstudio!\n","href":"/tutorials/installation/user/user_install_kerberos/","title":"Using Kerberos authentication for database connection"},{"content":" Naming  Use lowercase for repository, directory, and file names. For R-related files, use uppercase R.\nhttps://github.com/inbo/data-publication ../tutorials/gis/leaflet-R.Rmd  Use dash (-) to separate words in repository, directory, and file names. Don\u0026rsquo;t use underscores.\n.../datasets/bird-tracking-gull-occurrences/mapping/dwc-occurrence.sql   READMEs ","href":"/tutorials/tutorials/styleguide_repos/","title":"Styleguide new git repositories"},{"content":" Scope This style guide is a recommendation for all R code written for the Research Institute for Nature and Forest (INBO). The goal of this style guide is twofold. First of all applying the guidelines will result in readable code. Secondly, it is much easier to work together on code when everyone is using the same guidelines. It is likely that applying these guidelines will have consequences on the current style used by many R users at INBO. Therefore this style guide should be applied within reason.\n Don\u0026rsquo;t apply the style guide to existing code. R users are free to apply the style guide to new personal R code. Using the style guide is highly recommended for new or revised R code intended to be distributed and used by other R users. The style guide is mandatory for new or revised R packages distributed by INBO.  Please note that the RStudio editor has some handy features that automatically highlights errors against the code style in a non intrusive way. RStudio hints in this document are the instructions to activate these diagnostics in RStudio.\nSyntax RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Diagnotics: Check everything\nGeneral  lines should not exceed 80 characters  split the command over multiple lines if the command is longer than 80 characters RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Display: Check Show margin and set Margin column to 80  object names should be meaningful object names should not exceed 20 characters object names should be lowercase use _ to separate words in object names  function names with a single dot are allowed  use double quotes (\u0026quot;) around characters and not single quotes (') don\u0026rsquo;t add commented code  use version control if you want to keep old versions of code   # Good example_text \u0026lt;- example_function( first_argument = \u0026#34;Some text\u0026#34;, second_argument = \u0026#34;More text\u0026#34; ) # Bad some.really.long.dot.separated.name \u0026lt;- MyCoolFunction(FirstArgument = \u0026#39;Some text\u0026#39;, second.argument = \u0026#39;More text\u0026#39;) Whitespace naturallanguagesusewhitespaceandpunctuationtomaketextsmorereadableprogramminglanguagesarenoexceptiontothisrule\nNatural languages use whitespace and punctuation to make texts more readable. Programming languages are no exception to this rule.\n don\u0026rsquo;t use tabs, use two spaces instead  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Editing: Check Insert spaces for tab and set Tab width to 2  no space before a comma, one space after a comma one space before and after an infix operator (+, -, *, /, ^, \u0026amp;, |, %%, %/%, %*%, %in%, \u0026hellip;) no spaces a the end of a line  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Saving: Check Strip trailing horizontal whitespace when saving  end the script file with a single blank line  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Saving: Check Ensure that source files end with newline   Assignments  only create an object when you will use it later on always use \u0026lt;- for assignment use = only for passing arguments in a function at least one space before and at least one space after \u0026lt;- and =  use multiple spaces if it improves readability   # Good x \u0026lt;- data.frame(z = 1:10) summary(x) # Bad x=data.frame(z\u0026lt;-1:10) # Improved readability example a \u0026lt;- 5 ab \u0026lt;- 10 abc \u0026lt;- 7 d \u0026lt;- 245 Brackets R uses three types of brackets: round (...), square [...] and curly {...}.\n no spaces after opening a bracket no spaces before closing a bracket no spaces before opening a bracket except:  one space with control flow functions (if, else, for, while)  no spaces after closing a bracket except:  one space with control flow functions (if, else, for, while)  { should not start on a newline and is always the end of a line apply indentation when splitting long text inside brackets over multiple lines  # Good y \u0026lt;- seq(0, 2) if (max(y) \u0026lt;= 10) { x \u0026lt;- 1 } else { x \u0026lt;- 2 } sapply( y, function(x){ return(x) } ) # Bad y \u0026lt;- seq (0, 2 ) if( max( y ) \u0026lt;= 10 ) { x \u0026lt;- 1 } else { x \u0026lt;- 2 } sapply( y , function ( x ) { return(x)}) Special cases and exceptions  selecting rows with square brackets df[selection, ]  this results in two conflicting rules  a single space after a comma no space before a bracket  solution in case of a short command: add # nolint after the command  df[selection, ] # nolint  solution in case of a long command: split the command over several lines   # Good relevant_subset \u0026lt;- original_dataframe[ original_dataframe$x \u0026gt; some_value | original_dataframe$y \u0026lt; some_other_value, ] # Recommended dplyr alternative relevant_subset \u0026lt;- original_dataframe %\u0026gt;% filter(x \u0026gt; some_value | y \u0026lt; some_other_value) # Bad relevant_subset \u0026lt;- original_dataframe[original_dataframe$x \u0026gt; some_value | original_dataframe$y \u0026lt; some_other_value, ] # nolint  a really long text  text shorter than 80 characters but passed the 80 character limit due to the indentitation  solution: remove all indentation  text longer than 80 characters  solution: add # nolint at the end of the line   functions from other packages with names that don\u0026rsquo;t comply with this style guide  solution: add # nolint at the end of the line   Important notice\nAdding # nolint at the end of a line excludes that line from the automatic checks for coding styles. Therefore use it only when you have no other options.\nValidating syntax The code below validate the syntax for an R file, an RMarkdown file or an R package.\nRStudio hint: Running this code within RStudio will open a Markers pane, indicating the filename, line number and the kind of syntax error that occurs. Double clicking on the error will open the file a the correct location, making it easy to rectify the problem.\nExtra hint: start correcting for the last lines and work your way forward. This leave the line numbers of the errors intact until you solve them.\nAn example to clarify this. Suppose you have an error at line 10 and an error at line 100. Both errors are lines should not be more than 80 characters, so to solve them we have to split the lines over multiple lines.\nLet say that we start with solving line 10 by splitting it over four lines. So the old line 10 becomes the new lines 10 to 13. Hence the old line 11 becomes the new line 14, and the old line 100 becomes the new line 103. When we now click on the marker for line 100, RStudio will go the current line 100 which is the old line 97. So you end up looking for an error at the wrong position.\nStarting at the back solved this issue. In the same example we would start by solving line 100. Let\u0026rsquo;s assume we split this over two lines. So old line 100 because new line 100 and 101. Old line 101 becomes new line 102 but more importantly all line numbers before 100 are unchanged. So clicking on the marker for line 10 will take you the current line 10 which is the old line 10.\n# validate a single file lintr::lint(filename = \u0026#34;file.R\u0026#34;) lintr::lint(filename = \u0026#34;file.Rmd\u0026#34;) # validate a package lintr::lint_package(path = \u0026#34;.\u0026#34;) Documentation Functions  Add documentation above each function with Roxygen markup Add inline comments where relevant  Required Roxygen tags #\u0026#39; @title Title of the helpfile #\u0026#39; @description Description of the function in the helpfile #\u0026#39; @param define a parameter #\u0026#39; @export is the function exported by the package NAMESPACE #\u0026#39; @importFrom import a function from another package Optional Roxygen tags #\u0026#39; @seealso link to other functions #\u0026#39; @section section title #\u0026#39; @alias other name for the topic #\u0026#39; @keywords a set of standardised keywords. See file.path(R.home(\u0026#34;doc\u0026#34;), \u0026#34;KEYWORDS\u0026#34;) #\u0026#39; @inheritParams inherit the definition of parameters from another function #\u0026#39; @examples a working example of the function #\u0026#39; @return a description of the output from the function See http://r-pkgs.had.co.nz/man.html#roxygen-comments for more information on Roxygen\nINBO extra requirements for package DESCRIPTION  license: MIT or GPL-3? In case of MIT a LICENSE file should be added and License: MIT to the DESCRIPTION. In case of GPL-3 it is sufficient to add License: GPL-3 to the DESCRIPTION list of authors in Authors@R format INBO is listed as copyright holder one or more roles are atributed to each person  cre: package maintainer (only one person) aut: main author (at least one person) ctb: contributor (if relevant) cph: copyright holder (must be INBO)   Authors@R: c(person(\u0026ldquo;Els\u0026rdquo;, \u0026ldquo;Lommelen\u0026rdquo;, email = \u0026ldquo;els.lommelen@inbo.be\u0026rdquo;, role = c(\u0026ldquo;aut\u0026rdquo;, \u0026ldquo;cre\u0026rdquo;)), person(\u0026ldquo;Thierry\u0026rdquo;, \u0026ldquo;Onkelinx\u0026rdquo;, email = \u0026ldquo;thierry.onkelinx@inbo.be\u0026rdquo;, role = \u0026ldquo;aut\u0026rdquo;), person(\u0026ldquo;Anja\u0026rdquo;, \u0026ldquo;Leyman\u0026rdquo;, email = \u0026ldquo;anja.leyman@inbo.be\u0026rdquo;, role = \u0026ldquo;ctb\u0026rdquo;), person(family = \u0026ldquo;Research Institute for Nature and Forest (INBO)\u0026rdquo;, email = \u0026ldquo;info@inbo.be\u0026rdquo;, role = \u0026ldquo;cph\u0026rdquo;))\nHow-to\u0026rsquo;s  Add one or more how-to\u0026rsquo;s to a package Add them as RMarkdown vignettes  File structure R Package Functions  all generic R functions should be distributed as an R package use devtools::create() to start a new pacakge  RStudio hint: File \u0026gt; New project \u0026gt; New directory \u0026gt; R Package: Type the name in Package name  keep source files compact  create a separate file for each function, with the file name equal to the function name. This makes it easy to find the correct source file. exception: very short auxilary functions with related functionality  related functions can be bundled into one R script file name is either equal to the most important function or describes the related functionality   split large functions into several subfunctions  Scripts  place scripts in the inst folder  the scripts will be available for the user after installing the package the location of the scripts can be found with system.file(\u0026quot;script-name.R\u0026quot;, package = \u0026quot;yourpackage\u0026quot;) use a relevant folder structure when adding lots of files to inst   Unit tests  use the testthat package for unit tests  use devtools::use_testthat() to setup the test infrastructure  all unit tests are stored in tests/testthat all files should have either a test_ or helper_ prefix  files with helper_ prefix contain auxiliary function for the tests but no tests  the test files will be run in alphabetical order  setting the order of the files is easy by adding 3 letters to the prefix (eg. test_aaa_, test_baa, test_zzz_) 3 letters offers quite some flexibility to insert new files at the correct location without having to rename at lot of files. If the first file is test_aaa_ and the second test_baa, they you can 675 files between the two.  unit test files can be larger than source files a unit test file can contain tests for several functions in case the functions are strongly related (e.g. subfunctions) and reuse test cases each package should contain the unit for coding style as listed below  store this in a file tests/testthat/test_zzz_coding_style.R add this file to .gitignore  the coding style will be tested separatly when using continuous integration    if (requireNamespace(\u0026#34;lintr\u0026#34;, quietly = TRUE)) { context(\u0026#34;lints\u0026#34;) test_that(\u0026#34;Package Style\u0026#34;, { lintr::expect_lint_free() }) } R script  group a long set of commands with similar functionality into a dedicated function  e.g. prepare_data(), do_analysis(), create_figure(), \u0026hellip;  place the user defined functions in a separate file which you source() into the main script  it is better to use the same file structure as an R package consider writing a simple package in case you have a lot of functions   RMarkdown  each chunk has only one output (figure, table, summary, \u0026hellip;) don\u0026rsquo;t mix (heavy) calculations and output in the same chunk: this is more interesting for caching the results give chunks a relevant name: this make debugging easier and file name of figures and Bookdown label will be based on the chunk name avoid writing code that generates Markdown  use (parametrised) child documents instead  use the bookdown version for long reports: this makes it easy to split a long report into several child documents  Recommended packages Data import  readr: import text files readxl: import Excel files googlesheets: import Google Sheets DBI: connect to databases PostgreSQL, SQLite, MySQL, Oracle, \u0026hellip; RODBC: connect to databases SQL Server, Access  Data manipulation \u0026amp; transformation  dplyr:  subsetting observations subsetting variables changing variables aggregation combining dataframes  tidyr:  changing a dataframe from wide to long format and vice versa nesting and unnesting dataframes splitting a single variable into multiple variables   Graphics  ggplot2:all static graphics, charts and plots INBOtheme: INBO corporate identity for ggplot2 graphics  Quality control  lintr: checking coding style testthat: writing unit tests covr: check which part of the code is not covered by unit tests  ","href":"/tutorials/tutorials/styleguide_r_code/","title":"Styleguide R code"},{"content":" Na de eerste installatie  Start Rstudio Kies in het menu Tools -\u0026gt; Global Options In het tabblad General  Pas de Default working directory aan naar de locatie waar je R versie staat (C:/Program Files/R/R-3.x.y) 1 Restore .RData into workspace at startup: uitvinken Save workspace to.RData on exit: Never  In het tabblad Code  Subtab Editing  Insert spaces for tab: aanvinken Tab width: 2 Soft-wrap R source files: aanvinken  Subtab Saving  Default text encoding: UTF-8  Subtab Diagnostics  Alles aanvinken   In het tabblad Appearance  Stel in naar eigen smaak  In het tabblad Packages  CRAN mirror: wijzigen naar Global (CDN) - RStudio  In het tabblad Sweave  Weave Rnw files using: knitr Typeset LaTeX into PDF using: XeLaTex  Klik op OK en herstart RStudio  Configuratie van RStudio na een upgrade van R  Start RStudio Kies in het menu Tools -\u0026gt; Global Options Indien niet de laatste versie vermeld staat bij R version: klik op Change om het aan te passen. Klik op OK als je een waarschuwing krijgt dat je RStudio moet herstarten. Wijzig de initial working directory in C:/Program Files/R/R-3.x.y 1 Klik op OK Herstart RStudio   x en y verwijzen naar het versienummer. Dus bij R-3.1.0 is x = 1 en y = 0. De working directory is in dat geval C:/Progam Files/R/R-3.1.0 [return]   ","href":"/tutorials/installation/user/user_install_rstudio/","title":"Rstudio installation"},{"content":" Na de eerste installatie Bij een nieuwe R installatie hoeft de gebruiker geen bijkomende stappen te ondernemen.\nNa elke upgrade Voor onderstaande instructies uit telkens een nieuwe R versie geïnstalleerd werd. Je kan dit, indien gewenst, ook frequenter uitvoeren.\nWindows  Zorg dat de computer verbonden is met het internet. Zorg dat er geen enkele R sessie actief is op de computer. Start R x64 via het menu start. Tik het commando update.packages(ask = FALSE, checkBuilt = TRUE) gevolgd door enter. Er zullen nu packages waarvan een nieuwe versie beschikbaar is gedownload en geïnstalleerd worden. Dit duurt een hele poos, afhankelijk van het aantal te upgraden packages. Wacht tot de installatie volledig afgelopen is. Tik vervolgens het commando q() gevolgd door enter. R zal nu afgesloten worden.  Linux  Zorg dat de computer verbonden is met het internet. Zorg dat er geen enkele R sessie actief is op de computer. Open een terminal venster (Ctrl + Alt + T). Voer het commando Rscript -e 'update.packages(ask = FALSE, checkBuilt = TRUE)' uit. Er zullen nu packages waarvan een nieuwe versie beschikbaar is gedownload en geïnstalleerd worden. Dit duurt een hele poos, afhankelijk van het aantal te upgraden packages. Wacht tot de installatie volledig afgelopen is. Sluit de terminal met exit.  ","href":"/tutorials/installation/user/user_install_r/","title":"R language installation"},{"content":" Introduction You notice you have done something wrong on your branch. No worries, erroneous commits can be undone or reverted.\nFirst, check your history of commits to see which commits are the faulty ones.\ngit log --oneline  Example output\nb7119f2 Continue doing crazy things 872fa7e Try something crazy a1e8fb5 Make some important changes to hello.py 435b61d Create hello.py 9773e52 Initial import  Git checkout The git checkout command serves three distinct functions: checking out files, checking out commits, and checking out branches. In this part, only the first two configurations are addressed. Checking out a commit makes the entire working directory match that commit. This can be used to view an old state of your project without altering your current state in any way. Checking out a file lets you see an old version of that particular file, leaving the rest of your working directory untouched. (This will put you in a detached HEAD state.)\nYou can use git checkout to view the “Make some import changes to hello.py” commit as follows:\ngit checkout a1e8fb5  This makes your working directory match the exact state of the a1e8fb5 commit. You can look at files, and even edit files without worrying about losing the current state of the project. Nothing you do in here will be saved in your repository. Checking out an old commit is a read-only operation. It’s impossible to harm your repository while viewing an old revision. To continue developing, you need to get back to the “current” state of your project (assuming your master branch is the head of the project):\ngit checkout master  If you’re only interested in a single file, you can also use git checkout to fetch an old version of it. For example, if you only wanted to see the hello.py file from the old commit, you could use the following command:\ngit checkout a1e8fb5 hello.py  Remember, unlike checking out a commit, this does affect the current state of your project. The old file revision will show up as a “Change to be committed,” giving you the opportunity to revert back to the previous version of the file. If you decide you don’t want to keep the old version, you can check out the most recent version with the following:\ngit checkout HEAD hello.py  This concludes the part on checking your previous commits. In the following part, some methods of rollback to a previous state will be elucidated.\ngit revert The git revert command undoes a committed snapshot. But, instead of removing the commit from the project history, it figures out how to undo the changes introduced by the commit and appends a new commit with the resulting content. This prevents Git from losing history, which is important for the integrity of your revision history and for reliable collaboration.\nUsage:\ngit revert \u0026lt;commit\u0026gt;  Generate a new commit that undoes all of the changes introduced in , then apply it to the current branch.\nReverting should be used when you want to remove an entire commit from your project history. This can be useful, for example, if you’re tracking down a bug and find that it was introduced by a single commit. Instead of manually going in, fixing it, and committing a new snapshot, you can use git revert to automatically do all of this for you.\nIt\u0026rsquo;s important to understand that git revert undoes a single commit—it does not “revert” back to the previous state of a project by removing all subsequent commits. In Git, this is actually called a reset, not a revert.\nReverting has two important advantages over resetting. First, it doesn’t change the project history, which makes it a “safe” operation for commits that have already been published to a shared repository. For details about why altering shared history is dangerous, please see the git reset page.\nSecond, git revert is able to target an individual commit at an arbitrary point in the history, whereas git reset can only work backwards from the current commit. For example, if you wanted to undo an old commit with git reset, you would have to remove all of the commits that occurred after the target commit, remove it, then re-commit all of the subsequent commits. Needless to say, this is not an elegant undo solution.\nExample\nThe following example is a simple demonstration of git revert. It commits a snapshot, then immediately undoes it with a revert.\n// edit some tracked files // commit a snapshot git commit -m 'make some changes that will be undone' // revert the commit we have just created git revert HEAD  This can be visualized as the following:\nNote that the 4th commit is still in the project history after the revert. Instead of deleting it, git revert added a new commit to undo its changes. As a result, the 3rd and 5th commits represent the exact same code base, and the 4th commit is still in our history just in case we want to go back to it down the road.\ngit reset If git revert is a “safe” way to undo changes, you can think of git reset as the dangerous method. When you undo with git reset(and the commits are no longer referenced by any ref or the reflog), there is no way to retrieve the original copy—it is a permanent undo. Care must be taken when using this tool, as it’s one of the only Git commands that has the potential to lose your work.\nLike git checkout, git reset is a versatile command with many configurations. It can be used to remove committed snapshots, although it’s more often used to undo changes in the staging area and the working directory. In either case, it should only be used to undo local changes—you should never reset snapshots that have been shared with other developers.\nUsage:\ngit reset \u0026lt;file\u0026gt;  Remove the specified file from the staging area, but leave the working directory unchanged. This unstages a file without overwriting any changes.\ngit reset  Reset the staging area to match the most recent commit, but leave the working directory unchanged. This unstages all files without overwriting any changes, giving you the opportunity to re-build the staged snapshot from scratch.\ngit reset --hard  Reset the staging area and the working directory to match the most recent commit. In addition to unstaging changes, the \u0026ndash;hard flag tells Git to overwrite all changes in the working directory, too. Put another way: this obliterates all uncommitted changes, so make sure you really want to throw away your local developments before using it.\ngit reset \u0026lt;commit\u0026gt;  Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone. All changes made since  will reside in the working directory, which lets you re-commit the project history using cleaner, more atomic snapshots.\ngit reset --hard \u0026lt;commit\u0026gt;  Move the current branch tip backward to  and reset both the staging area and the working directory to match. This obliterates not only the uncommitted changes, but all commits after , as well.\nDiscussion\nAll of the above invocations are used to remove changes from a repository. Without the \u0026ndash;hard flag, git reset is a way to clean up a repository by unstaging changes or uncommitting a series of snapshots and re-building them from scratch. The \u0026ndash;hard flag comes in handy when an experiment has gone horribly wrong and you need a clean slate to work with.\nWhereas reverting is designed to safely undo a public commit, git reset is designed to undo local changes. Because of their distinct goals, the two commands are implemented differently: resetting completely removes a changeset, whereas reverting maintains the original changeset and uses a new commit to apply the undo.\nDon’t Reset Public History\nYou should never use git reset  when any snapshots after  have been pushed to a public repository. After publishing a commit, you have to assume that other developers are reliant upon it.\nRemoving a commit that other team members have continued developing poses serious problems for collaboration. When they try to sync up with your repository, it will look like a chunk of the project history abruptly disappeared. The sequence below demonstrates what happens when you try to reset a public commit. The origin/master branch is the central repository’s version of your local master branch.\nAs soon as you add new commits after the reset, Git will think that your local history has diverged from origin/master, and the merge commit required to synchronize your repositories is likely to confuse and frustrate your team.\nThe point is, make sure that you’re using git reset  on a local experiment that went wrong—not on published changes. If you need to fix a public commit, the git revert command was designed specifically for this purpose.\nExamples: unstaging a file The git reset command is frequently encountered while preparing the staged snapshot. The next example assumes you have two files called hello.py and main.py that you’ve already added to the repository.\n# Edit both hello.py and main.py # Stage everything in the current directory git add . # Realize that the changes in hello.py and main.py # should be committed in different snapshots # Unstage main.py git reset main.py # Commit only hello.py git commit -m \u0026quot;Make some changes to hello.py\u0026quot; # Commit main.py in a separate snapshot git add main.py git commit -m \u0026quot;Edit main.py\u0026quot;  As you can see, git reset helps you keep your commits highly-focused by letting you unstage changes that aren’t related to the next commit.\nremoving local commits The next example shows a more advanced use case. It demonstrates what happens when you’ve been working on a new experiment for a while, but decide to completely throw it away after committing a few snapshots.\n# Create a new file called `foo.py` and add some code to it # Commit it to the project history git add foo.py git commit -m \u0026quot;Start developing a crazy feature\u0026quot; # Edit `foo.py` again and change some other tracked files, too # Commit another snapshot git commit -a -m \u0026quot;Continue my crazy feature\u0026quot; # Decide to scrap the feature and remove the associated commits git reset --hard HEAD~2  The git reset HEAD~2 command moves the current branch backward by two commits, effectively removing the two snapshots we just created from the project history. Remember that this kind of reset should only be used on unpublished commits. Never perform the above operation if you’ve already pushed your commits to a shared repository.\nUsage statement The content of this Rmarkdown tutorial is a transformation from this source and is licensed under a Creative Commons Attribution 2.5 Australia License.\n","href":"/tutorials/tutorials/git_undo_commit/","title":"Undo a git commit"},{"content":" When working off line, two Git tasks cannot be performed: fetching/pulling updates from the server, and pushing changes to the server. All other commands still work.\nOne can commit changes, branch off, revert and reset changes, the same as when there exists an internet connection.\nExample workflow: start offline mode\nwhile(notBored): commit changes add files branch off new features  end offline mode\nupdate master branch\ngit fetch origin  push changes to the server\ngit push \u0026lt;branch-name\u0026gt;  it is possible, that during your down-time, a pull request got accepted in that case, perform the following steps\ngit fetch origin git checkout \u0026lt;branch-name\u0026gt; git rebase master  when necessary: solve merge conflicts, and rebase again.\nYour feature branch can now be pushed to the server, and a pull request can be made\n","href":"/tutorials/tutorials/git_no_internet/","title":"Git without internet"},{"content":" Fix merge conflict with a pull request You have made some changes to a feature branch. Make a pull request on the server. The standard case of automatic merge is not possible. Push your latest changes from the feature branch to the server.\nLocally on your computer:\ngit fetch origin  Rebase your feature branch with your master\ngit rebase origin/master  Git will now state that there are merge conflicts. These will look like this:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD:\u0026lt;some git nonsense\u0026gt; This part is from a version of this file ===== This is from another version of a file \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; blahdeblahdeblah:\u0026lt;some more git nonsense\u0026gt;  The \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;, ===== and \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; markers show which lines were changed simultaneously. In order to remove the conflict, choose which line you want to keep (first or second), remove the other line and the markers, and finally commit the result.\nAdd your files that you fixed.\ngit add \u0026lt;fixed files\u0026gt;  Continue with your rebase\ngit rebase --continue  If more troubles occur, fix them, add them, and do a git rebase continue\nForce push your branch to the server. (force because you changed the commit)\ngit push -f origin branchname  On the server, you can now automatically close your Pull Request.\n","href":"/tutorials/tutorials/git_conflict/","title":"Handle conflicts"},{"content":" BEFORE I START WORKING  STEP 1: Update the master branch on my PC to make sure it is aligned with the remote master\ngit fetch origin git checkout master git merge --ff-only origin/master  STEP 2: Choose your option:\n OPTION 2A: I already have a branch I want to continue working on:\nSwitch to existing topic branch:\ngit checkout name_existing_branch git fetch origin git rebase origin/master  OPTION 2B: I\u0026rsquo;ll make a new branch to work with: Create a new topic branch from master(!):\ngit checkout master git checkout -b name_new_branch    WHILE EDITING  STEP 3.x: adapt in tex, code,\u0026hellip; (multiple times)\n New files added\ngit add .  Adaptation\ngit commit -am \u0026quot;clear and understandable message about edits\u0026quot;    EDITS ON BRANCH READY  STEP 4: Pull request to add your changes to the current master. Choose your option:\n OPTION 2A CHOSEN:\ngit push origin name_existing_branch  OPTION 2B CHOSEN:\ngit push origin name_new_branch   STEP 5: Code review!\nGo to your repo on Github.com and click the create pull request block. You and collaborators can make comments about the edits and review the code.\nIf everything is ok, click the Merge pull request, followed by confirm merge. (all online actions on GitHub). Delete the online branch, since obsolete.\nYou\u0026rsquo;re work is now tracked and added to master! Congratulations.\nIf the code can\u0026rsquo;t be merged automatically (provided by a message online), go to STEP 6.\n  PULL REQUEST CANNOT BE MERGED BY GITHUB  STEP 6: master has changed and there are conflicts: update your working branch with rebase\ngit checkout name_existing_branch git fetch origin git rebase origin/master # fix conflicts local git add file_with_conflict git rebase --continue git push -f origin name_existing_branch   ","href":"/tutorials/tutorials/git_workflow/","title":"Git workflow"},{"content":"The list below contains all R related software which should be installed on the computers of useRs at INBO. Note that the installation process of most software requires administrator rights.\n","href":"/tutorials/installation/administrator/","title":"Administrator installation notes"},{"content":"Here a some pages which describe the steps that the users stil needs to do after an installation or upgrade.\n","href":"/tutorials/installation/user/","title":"User installation notes"},{"content":" De installatiebestanden voor de stabiele versies zijn beschikbaar via http://www.rstudio.com/products/rstudio/download/. De preview versie is beschikbaar via https://www.rstudio.com/products/rstudio/download/preview/\nWindows Nieuwe installatie en upgrade van RStudio RStudio upgraden doe je door de nieuwe versie te installeren over de oude.\n Zorg dat eerst R geïnstalleerd is. Voer het 64-bit installatiebestand uit. Welkom bij de installatie: klik op volgende. Geef de doelmap en klik op volgende. Je mag de standaard gebruiken. Klik op installeren. Klik op voltooien.  RStudio mag niet met admininstratorrechten gestart worden. Anders worden een aantal R packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nTest de configuratie door RStudio te starten als een gewone gebruiker.\nAfwijkingen t.o.v. default installatie  Geen  ","href":"/tutorials/installation/administrator/admin_install_rstudio/","title":"RStudio Desktop installation"},{"content":" Windows Installatie en upgrade De installatiebestanden zijn beschikbaar via https://cloud.r-project.org/bin/windows/Rtools/\nBij een upgrade dient eerst de vorige versie te worden verwijderd.\n Kies Nederlands als taal voor de installatie en klik volgende. Klik volgende. Kies C:\\Rtools (standaard) als installatiemap en klik volgende. Kies uit dropdown menu Tools for building R packages from source (standaard) en klik volgende. Vink Add rtools to system path aan en klik volgende. Klik volgende. Klik installeren. Klik voltooien.  Afwijkingen t.o.v. default installatie  Aanvinken van Add rtools to system path  ","href":"/tutorials/installation/administrator/admin_install_rtools/","title":"Rtools installation"},{"content":" Windows De installatiebestanden zijn beschikbaar via http://git-scm.com/downloads\n Voer het installatiebestand uit Welkom bij de installatie: klik op Next Aanvaard de licentievoorwaarden door Next te klikken Installeer git in de voorgesteld standaard directory. Gebruik de standaard componenten door Next te klikken Kies use Git from the Windows Command Prompt en klik Next Kies Checkout Windows-style, commit Unix-style line endings en klik Next Kies use Windows' default console window en klik Next Klik op Finish  Afwijkingen t.o.v. default installatie  use Git from the Windows Command Prompt use Windows' default console window  Ubuntu sudo apt-get update sudo apt-get install git  ","href":"/tutorials/installation/administrator/admin_install_git/","title":"Git installation"},{"content":" Windows Pandoc wordt automatisch geïnstalleerd als je RStudio installeert.\nUbuntu  Kijk op https://github.com/jgm/pandoc/releases wat de laatste versie is. Pas het versienummer in onderstaande code aan en voer ze uit in een terminalvenster\nwget https://github.com/jgm/pandoc/releases/download/1.19/pandoc-1.19-1-amd64.deb sudo dpkg -i pandoc-1.19-1-amd64.deb rm pandoc-1.19-1-amd64.deb   ","href":"/tutorials/installation/administrator/admin_install_pandoc/","title":"Pandoc installation"},{"content":" Windows Installatiebestand beschikbaar via https://cloud.r-project.org/bin/windows/base/\nIn de onderstaande tekst moet je in R-3.x.y zowel x als y vervangen door een cijfer om zo het huidige versienummer te krijgen. Dus voor versie R-3.0.0 is x = 0 en y = 0.\nNieuwe installatie van R  Voer het bestand R-3.x.y-win.exe uit. Kies Nederlands als taal voor de installatie en klik op OK. klik op Volgende. Aanvaard de licentievoorwaarden door op Volgende te klikken. Gebruik de standaarddoelmap en klik op Volgende. Selecteer de gewenste componenten en klik op Volgende. Je MOET deze standaardwaarden laten staan. Opstartinstelling aanpassen: Kies Nee en klik op Volgende. Geef de map voor het start menu en klik op Volgende. Je mag de standaardwaarde gebruiken. Vink de gewenste extra snelkoppelingen aan (default is ok), alle register entries aan en klik op Volgende. R wordt nu geïnstalleerd. Klik op Voltooien als de installatie afgelopen is. Ga naar Start en tik \u0026ldquo;Omgevingsvariabelen\u0026rdquo; in het veld Programma's en variabelen zoeken. Selecteer De omgevingsvariabelen van het systeem bewerken. Selecteer het tabblad Geavanceerd en klik op de knop Omgevingsvariabelen. Ga na of er een systeemvariabele R_LIBS_USER met waarde C:/R/library bestaat[1]. Indien niet, maak deze aan met de knop Nieuw. Sluit al deze schermen via de OK knop. Kopieer het bestand Rprofile.site naar etc in de doelmap waar je R geïnstalleerd hebt (C:\\Program Files\\R\\R-3.x.y) Hierbij moet je het bestaande bestand overschrijven. Zorg dat de gebruiker schrijfrechten heeft voor C:\\Program Files\\R\\R-3.x.y\\library en C:\\R\\library  Afwijkingen t.o.v. default installatie  alle gebruikers moeten volledige rechten hebben in  C:\\R\\library C:\\Program Files\\R\\R-3.x.y\\library  Systeemvariable R_LIBS_USER instellen op C:/R/library (verplicht forward slashes) Rprofile.site in C:\\Program Files\\R\\R-3.x.y\\etc overschrijven  R mag niet met admininstratorrechten gestart worden. Anders worden een aantal packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nStart R als een gewone gebruiker om de configuratie te testen.\nUpgrade van een bestaande R installatie Deze instructies veronderstellen dat R en RStudio in het verleden reeds geïnstalleerd werden volgens de bovenstaande instructies. Indien dan niet het geval is, volg dan de instructies voor een nieuwe installatie.\n Voer het bestand R-3.x.y-win.exe uit. Kies Nederlands als taal voor de installatie en klik op OK. klik op Volgende. Aanvaard de licentievoorwaarden door op Volgende te klikken. Gebruik de standaarddoelmap en klik op Volgende. Selecteer de gewenste componenten en klik op Volgende. Je MOET deze standaardwaarden laten staan. Opstartinstelling aanpassen: Kies Nee en klik op Volgende. Geef de map voor het start menu en klik op Volgende. Je mag de standaardwaarde gebruiken. Vink de gewenste extra snelkoppelingen aan (default is ok), alle register entries aan en klik op Volgende. R wordt nu geïnstalleerd. Klik op Voltooien als de installatie afgelopen is. Kopieer het bestand Rprofile.site naar etc in de doelmap waar je R geïnstalleerd hebt (C:\\Program Files\\R\\R-3.x.y) Hierbij moet je het bestaande bestand overschrijven. Zorg dat de gebruiker schrijfrechten heeft voor C:\\Program Files\\R\\R-3.x.y\\library De nieuwe R versie is klaar voor gebruik. De gebruiker moet RStudio bijwerken.  R mag niet met admininstratorrechten gestart worden. Anders worden een aantal packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nStart R als een gewone gebruiker om de configuratie te testen.\nInhoud Rprofile.site options( papersize = \u0026quot;a4\u0026quot;, tab.width = 2, width = 80, help_type = \u0026quot;html\u0026quot;, stringsAsFactors = TRUE, keep.source.pkgs = TRUE, xpinch = 300, ypinch = 300, yaml.eval.expr = TRUE, repos = c( RStudio = \u0026quot;http://cloud.r-project.org/\u0026quot;, INLA = \u0026quot;http://inla.r-inla-download.org/R/stable\u0026quot; ), install.packages.check.source = \u0026quot;no\u0026quot;, install.packages.compile.from.source = \u0026quot;never\u0026quot; ) if (interactive()) { if (length(find.package(\u0026quot;drat\u0026quot;, quiet = TRUE)) == 0) { utils::install.packages(\u0026quot;drat\u0026quot;) } drat::addRepo(\u0026quot;inbo\u0026quot;) if (length(find.package(\u0026quot;fortunes\u0026quot;, quiet = TRUE)) == 0) { utils::install.packages(\u0026quot;fortunes\u0026quot;) } tryCatch( print(fortunes::fortune()), error = function(e){ invisible(NULL) } ) } # required for RStan and brms Sys.setenv(BINPREF = \u0026quot;C:/Rtools/mingw_$(WIN)/bin/\u0026quot;)  Ubuntu sudo sh -c 'echo \u0026quot;deb http://cloud.r-project.org/bin/linux/ubuntu xenial/\u0026quot; \u0026gt;\u0026gt; /etc/apt/sources.list' sudo gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9 sudo gpg -a --export E084DAB9 | apt-key add - sudo apt-get update sudo apt-get install -y r-base r-base-dev libcurl4-openssl-dev libssl-dev libssh2-1-dev libxml2-dev  [1] Het moeten forward slashes zijn.\n","href":"/tutorials/installation/administrator/admin_install_r/","title":"Install R"},{"content":" R CMD check has a large set of generic quality tests on a package. It is impossible to create generic tests that check the content of the package. E.g. does each function return sensible results. However, R CMD check does run a set unit tests. These are small pieces of code written by the package developer which test the output of a specific function under specific circumstances. We highly recommend the testthat framework for writing unit tests.\nCombining code coverage and Wercker A useful tool to visualise the coverage of the package by unit tests, is codecov. It can be added to the Wercker application by:\n login to http://www.codecov.io (via GitHub) and copy the token add it to the tab Environment on http://www.wercker.com/: Key = CODECOV_TOKEN, Value = (paste the token) and tick \u0026lsquo;Protected\u0026rsquo; to prevent it from being viewed. This makes it secure.  Note that it only makes sense when the wercker.yaml has a inbobmk/r-coverage or jimhester/r-coverage step.\n","href":"/tutorials/tutorials/development_codecov/","title":"Code coverage"},{"content":" Intro R is known to have difficulties handling large data files. Here we will explore some tips that make working with such files in R less painfull.\ntl;dr  If you can comfortably work with the entire file in memory, but reading the file is rather slow, consider using the data.table package and read the file with its fread function. If your file does not comfortably fit in memory:  Use sqldf if you have to stick to csv files. Use a SQLite database and query it using either SQL queries or dplyr. Convert your csv file to a sqlite database in order to query   Downloading the example files While you can directly test this tutorial on your own large data files, we will use bird tracking data from the LifeWatch bird tracking network for the examples. We have made two versions of some tracking data available for download: a .csv file (text data) and a .db file (sqlite data). Both contain processed log files; for more information on the processing, see the BirdTrackingEtl package.\ncsv.name \u0026lt;- \u0026#34;2016-04-20-processed-logs-big-file-example.csv\u0026#34; db.name \u0026lt;- \u0026#34;2016-04-20-processed-logs-big-file-example.db\u0026#34; The evaluation of the next code chunk is ignored by default as the downloading and unzipping of the files results in more than 3 GB of data. If you do want to download the files yourself and test the other chunks, run the code and download the csv and sqlite examples. Make sure you have the R.utils package available (for unzipping the downloaded files). If not, use the command install.packages(\u0026quot;R.utils\u0026quot;) in your R console to download the package.\nlibrary(\u0026#34;R.utils\u0026#34;) # download the CSV file example csv.url \u0026lt;- paste(\u0026#34;https://s3-eu-west-1.amazonaws.com/lw-birdtracking-data/\u0026#34;, csv.name, \u0026#34;.gz\u0026#34;, sep = \u0026#34;\u0026#34;) if (!file.exists(csv.name)) { download.file(csv.url, destfile = paste0(csv.name, \u0026#34;.gz\u0026#34;)) gunzip(paste0(csv.name, \u0026#34;.gz\u0026#34;)) } # download the sqlite database example db.url \u0026lt;- paste(\u0026#34;https://s3-eu-west-1.amazonaws.com/lw-birdtracking-data/\u0026#34;, db.name, \u0026#34;.gz\u0026#34;, sep = \u0026#34;\u0026#34;) if (!file.exists(db.name)) { download.file(db.url, destfile = paste0(db.name, \u0026#34;.gz\u0026#34;)) gunzip(paste0(db.name, \u0026#34;.gz\u0026#34;)) } Loading a large dataset: use fread or readr instead of read. library(\u0026#34;data.table\u0026#34;) library(\u0026#34;readr\u0026#34;) If you really need to read an entire csv in memory, by default, R users use the read.table method or variations thereof (such as read.csv). However, fread from the data.table package is a lot faster. Furthermore, the readr package also provides more optimized reading functions (read_csv, read_delim,\u0026hellip;). Let\u0026rsquo;s measure the time to read in the data using these three different methods.\nread.table.timing \u0026lt;- system.time(read.table(csv.name, header = TRUE, sep = \u0026#34;,\u0026#34;)) readr.timing \u0026lt;- system.time(read_delim(csv.name, \u0026#34;,\u0026#34;, col_names = TRUE)) data.table.timing \u0026lt;- system.time(allData \u0026lt;- fread(csv.name, showProgress = FALSE)) data \u0026lt;- data.frame(method = c(\u0026#39;read.table\u0026#39;, \u0026#39;readr\u0026#39;, \u0026#39;fread\u0026#39;), timing = c(read.table.timing[3], readr.timing[3], data.table.timing[3])) data ## method timing ## 1 read.table 362.819 ## 2 readr 34.016 ## 3 fread 24.785  fread and read_delim are indeed much faster then the default read.table. However, the result of fread is a data.table and the result of read_delim is a tibble. Both are not a data.frame. The data.table package describes the data.table object as a more performant replacement for the data.frame. This means that selecting, filtering and aggregating data is much faster on a data.table compared to the standard data.frame but it requires you to use a slightly different syntax. A tibble is very similar to a data.frame, but provides more convenience when printing or subsetting the data table.\nYou can find the data.table package on CRAN. A good place to learn this package are the package vignettes. The introduction to data.table should be enough to get started. You can also find the The readr package is also on CRAN. It belongs to a suite of R packages aiming to improve data manipulation in R, called tidyverse. More examples and explanation about readr is provided on the readr website.\nData files that don\u0026rsquo;t fit in memory If you are not able to read in the data file, because it does not fit in memory (or because R becomes too slow when you load the entire dataset), you will need to limit the amount of data that will actually be stored in memory. There are a couple of options which we will investigate:\n limit the number of lines you are trying to read for some exploratory analysis. Once you are happy with the analysis you want to run on the entire dataset, move to another machine. limit the number of columns you are reading to reduce the memory required to store the data. limit both the number of rows and the number of columns using sqldf. stream the data.  1. Limit the number of lines you read (fread) Limiting the number of lines you read is easy. Just use the nrows and/or skip option (available to both read.table and fread). skip can be used to skip a number of rows, but you can also pass a string to this parameter causing fread to only start reading lines from the first line matching that string. Let\u0026rsquo;s say we only want to start reading lines after we find a line matching the pattern 801,2014-06-29. We can do that like this:\nsprintf(\u0026#34;Number of lines in full data set: %s\u0026#34;, nrow(allData)) ## [1] \u0026quot;Number of lines in full data set: 3761058\u0026quot;  subSet \u0026lt;- fread(csv.name, skip = \u0026#34;2015-06-12 15:14:39\u0026#34;, showProgress = FALSE) sprintf(\u0026#34;Number of lines in data set with skipped lines: %s\u0026#34;, nrow(subSet)) ## [1] \u0026quot;Number of lines in data set with skipped lines: 9998\u0026quot;  Skipping rows this way is obviously not giving you the entire dataset, so this strategy is only useful for doing exploratory analysis on a subset of your data. Note that also read_delim provides a n_max argument to limit the number of lines to read. If you want to explore the whole dataset, limiting the number of columns you read can be a more useful strategy.\n2. Limit the number of columns you read (fread) If you only need 4 columns of the 21 columns present in the file, you can tell fread to only select those 4. This can have a major impact on the memory footprint of your data. The option you need for this is: select. With this, you can specify a number of columns to keep. The opposite - specifying the columns you want to drop - can be accomplished with the drop option.\nfourColumns = fread(csv.name, select = c(\u0026#34;device_info_serial\u0026#34;, \u0026#34;date_time\u0026#34;, \u0026#34;latitude\u0026#34;, \u0026#34;longitude\u0026#34;), showProgress = FALSE) sprintf(\u0026#34;Size of total data in memory: %s MB\u0026#34;, utils::object.size(allData)/1000000) ## [1] \u0026quot;Size of total data in memory: 1434.074264 MB\u0026quot;  sprintf(\u0026#34;Size of only four columns in memory: %s MB\u0026#34;, utils::object.size(fourColumns)/1000000) ## [1] \u0026quot;Size of only four columns in memory: 365.90692 MB\u0026quot;  The difference might not be as large as you would expect. R objects claim more memory then needed to store the data alone, as they keep pointers, and other object attributes. But still, the difference could save you.\n3. Limiting both the number of rows and the number of columns using sqldf The sqldf package allows you to run SQL-like queries on a file, resulting in only a selection of the file being read. It allows you to limit both the number of lines and the number of rows at the same time. In the background, this actually creates a sqlite database on the fly to execute the query. Consider using the package when starting from a csv file, but the actual strategy boils down to making a sqlite database file of your data. See this section below to learn how to interact with those and create a SQlite database from a CSV-file.\n4. Streaming data Short: streaming a file in R is a bad idea. If you are interested why, read the rest of this section.\nStreaming a file means reading it line by line and only keeping the lines you need or do stuff with the lines while you read through the file. It turns out that R is really not very efficient in streaming files. The main reason is the memory allocation process that has difficulties with a constantly growing object (which can be a dataframe containing only the selected lines).\nIn the next code block, we will read parts of our data file once using the freadfunction, and once line by line. You\u0026rsquo;ll see the performance issue with the streaming solution.\nlibrary(ggplot2) allowedDevices = c(753, 801, 852) minDate = strptime(\u0026#39;1/3/2014\u0026#39;,format = \u0026#39;%d/%m/%Y\u0026#39;) maxDate = strptime(\u0026#39;1/10/2014\u0026#39;,format = \u0026#39;%d/%m/%Y\u0026#39;) streamFile \u0026lt;- function(limit) { con \u0026lt;- file(csv.name, open = \u0026#34;r\u0026#34;) selectedRecords \u0026lt;- list() i \u0026lt;- 0 file.streaming.timing \u0026lt;- system.time( while (i \u0026lt; limit) { oneLine \u0026lt;- readLines(con, n = 1, warn = FALSE) vec = (strsplit(oneLine, \u0026#34;,\u0026#34;)) selectedRecords \u0026lt;- c(selectedRecords, vec) i \u0026lt;- i + 1 } ) close(con) return(file.streaming.timing[[3]]) } freadFile \u0026lt;- function(limit) { file.fread.timing = system.time( d \u0026lt;- fread(csv.name, showProgress = FALSE, nrows = limit) ) return(file.fread.timing[[3]]) } maxLines \u0026lt;- c(5000, 10000, 15000, 20000, 25000, 30000) streamingTimes \u0026lt;- sapply(maxLines, streamFile) freadTimes \u0026lt;- sapply(maxLines, freadFile) data \u0026lt;- data.frame(n = maxLines, streaming = streamingTimes, fread = freadTimes) pdata \u0026lt;- melt(data, id = c(\u0026#34;n\u0026#34;)) colnames(pdata) \u0026lt;- c(\u0026#34;n\u0026#34;, \u0026#34;algorithm\u0026#34;, \u0026#34;execTime\u0026#34;) qplot(n, execTime, data = pdata, color = algorithm, xlab = \u0026#34;number of lines read\u0026#34;, ylab = \u0026#34;execution time (s)\u0026#34;) The database file strategy Working with SQLite databases SQLite databases are single file databases meaning you can simply download them, store them in a folder or share them with colleagues. Similar to a csv. They are however more powerful than csv\u0026rsquo;s because of two important features:\n Support for SQL: this allows you to execute intelligent filters on your data, similar to the sqldf package or database environments you are familiar with. That way, you can reduce the amount of data that\u0026rsquo;s stored in memory by filtering out rows or columns. Indexes: SQLite databases contain indexes. An index is something like an ordered version of a column. When enabled on a column, you can search through the column much faster. We will demonstrate this below.  We have downloaded a second file 2016-04-20-processed-logs-big-file-example.db that contains the same data as the 2016-04-20-processed-logs-big-file-example.csv file, but as a sqlite database. Furthermore, the database file contains indexes which will dramatically drop the time needed to perform search queries. If you do not have a SQLite database containing your data, you can first convert your csv into a SQlite as described further in this tutorial.\nLet\u0026rsquo;s first connect to the database and list the available tables.\nlibrary(RSQLite) db \u0026lt;- dbConnect(SQLite(), dbname = db.name) # show the tables in this database dbListTables(db) ## [1] \u0026quot;SpatialIndex\u0026quot; ## [2] \u0026quot;geom_cols_ref_sys\u0026quot; ## [3] \u0026quot;geometry_columns\u0026quot; ## [4] \u0026quot;geometry_columns_auth\u0026quot; ## [5] \u0026quot;geometry_columns_field_infos\u0026quot; ## [6] \u0026quot;geometry_columns_statistics\u0026quot; ## [7] \u0026quot;geometry_columns_time\u0026quot; ## [8] \u0026quot;processed_logs\u0026quot; ## [9] \u0026quot;spatial_ref_sys\u0026quot; ## [10] \u0026quot;spatialite_history\u0026quot; ## [11] \u0026quot;sql_statements_log\u0026quot; ## [12] \u0026quot;sqlite_sequence\u0026quot; ## [13] \u0026quot;vector_layers\u0026quot; ## [14] \u0026quot;vector_layers_auth\u0026quot; ## [15] \u0026quot;vector_layers_field_infos\u0026quot; ## [16] \u0026quot;vector_layers_statistics\u0026quot; ## [17] \u0026quot;views_geometry_columns\u0026quot; ## [18] \u0026quot;views_geometry_columns_auth\u0026quot; ## [19] \u0026quot;views_geometry_columns_field_infos\u0026quot; ## [20] \u0026quot;views_geometry_columns_statistics\u0026quot; ## [21] \u0026quot;virts_geometry_columns\u0026quot; ## [22] \u0026quot;virts_geometry_columns_auth\u0026quot; ## [23] \u0026quot;virts_geometry_columns_field_infos\u0026quot; ## [24] \u0026quot;virts_geometry_columns_statistics\u0026quot;  Let\u0026rsquo;s try to select rows where the device id matches a given value (e.g. 860), and the date time is between two given timestamps. For our analysis, we only need date_time, latitude, longitude and altitude so we will only select those.\nsqlTiming \u0026lt;- system.time(data \u0026lt;- dbGetQuery(conn = db, \u0026#34;SELECT date_time, latitude, longitude, altitude FROM processed_logs WHERE device_info_serial = 860 AND date_time \u0026lt; \u0026#39;2014-07-01\u0026#39; AND date_time \u0026gt; \u0026#39;2014-03-01\u0026#39;\u0026#34; )) print(sqlTiming[3]) ## elapsed ## 13.48  This provides a convenient and fast way to request subsets of data from our large data file. We could do the same analysis for each of the serial numbers, each time only loading that subset of the data. As an example, consider the calculation of the average altitude over the specified period for each of the bird serial identifiers in the list serial_id_list. By using a for loop, the calculation is done for each of the birds separately and the amount of data loaded into memory at the same time is lower:\nserial_id_list \u0026lt;- c(853, 860, 783) print(\u0026#34;Average altitude between 2014-03-01 and 2014-07-01:\u0026#34;) ## [1] \u0026quot;Average altitude between 2014-03-01 and 2014-07-01:\u0026quot;  for (serialid in serial_id_list) { data \u0026lt;- dbGetQuery(conn = db, sprintf(\u0026#34;SELECT date_time, latitude, longitude, altitude FROM processed_logs WHERE device_info_serial = %d AND date_time \u0026lt; \u0026#39;2014-07-01\u0026#39; AND date_time \u0026gt; \u0026#39;2014-03-01\u0026#39;\u0026#34;, serialid)) print(sprintf(\u0026#34;serialID %d: %f\u0026#34;, serialid, mean(data$altitude))) } ## [1] \u0026quot;serialID 853: NA\u0026quot; ## [1] \u0026quot;serialID 860: 23.550518\u0026quot; ## [1] \u0026quot;serialID 783: 14.900030\u0026quot;  Remark that we use the sprintf function to dynamically replace the serial id in the sqlite query we will execute. For each loop, the %d is replaced by the value of the serial id of the respective loop. Read the manual of the sprintf function for more information and options.\nInteracting with SQLite databases using dplyr If you\u0026rsquo;re not comfortable with writing queries in SQL, R has a great alternative: dplyr. dplyr can connect to a SQLite database and you can perform the same operations on it that you would do on a dataframe. However, dplyr will translate your commands to SQL, allowing you to take advantage of the indexes in the SQLite database.\nlibrary(dplyr) my_db \u0026lt;- src_sqlite(db.name, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;processed_logs\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.19.3 ## # [/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/r_large_data_files_handling/2016-04-20-processed-logs-big-file-example.db] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  Dplyr provides the ability to perform queries as above without the need to know SQL. If you want to learn more about how to use dplyr with a SQLite database, head over to this vignette.\nCreate a SQLite databases from a CSV file In the case you have a CSV file available and you would like to query the data using SQL queries or with dplyr as shown in the previous sections, you can decide to convert the data to a SQlite database. The conversion will require some time, but once available, it provides the opportunity to query the data using SQL queries or with dplyr as shown in the previous sections. Moreover, you can easily add additional tables with related information to combine the data with.\nIf you already loaded the CSV file into memory, the creation of a SQLITE database is very straighforward and can be achieved in two steps:\ndb \u0026lt;- dbConnect(SQLite(), dbname = \u0026#34;example.sqlite\u0026#34;) dbWriteTable(db, \u0026#34;birdtracks\u0026#34;, allData) dbDisconnect(db) The first command creates a new database when the file example.sqlite does not exist already. The command dbWriteTable writes the table in the database. Hence, we can rerun the query from the previous section, but now on the newly created SQlite database, with the single created table birdtracks:\nmy_db \u0026lt;- src_sqlite(\u0026#34;example.sqlite\u0026#34;, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;birdtracks\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.19.3 ## # [/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/r_large_data_files_handling/example.sqlite] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  However, when working with really large CSV files, you do not want to load the entire file into memory first (this is the whole point of this tutorial). An alternative strategy is to load the data from the CSV file in chunks (small sections) and write them step by step to the SQlite database.\nThis can be implemented by reading the CSV file in small sections (let\u0026rsquo;s say 50000 lines each time) and move all sections to a given table in a sqlite database. As this is a recurrent task, we will provide the transformation in a custom written function, called csv_to_sqlite. The function is available within the inborutils package. Check the function documentation online or by typing ?csv_to_sqlite after installing and loading the inborutils package. As SQlite does not natively support date and datetime representations, the function converts those columns to an appropriate string representation before copying the dates to sqlite. To check for the date handling, the lubridate package is used.\nAs an example, let\u0026rsquo;s convert the processed bird logs csv file to a sqlite database, called example.sqlite as a table birdtracks. Using the default values for the preprocessing number of lines and the chunk size, the conversion is as follows:\nlibrary(inborutils) sqlite_file \u0026lt;- \u0026#34;example2.sqlite\u0026#34; table_name \u0026lt;- \u0026#34;birdtracks\u0026#34; inborutils::csv_to_sqlite(csv_file = csv.name, sqlite_file, table_name, pre_process_size = 1000, chunk_size = 50000, show_progress_bar = FALSE) Hence, this approach will work for large files as well and is an ideal first step when doing this kind of analysis. Once performed, the SQlite database is available to query, similar to the previous examples:\nmy_db \u0026lt;- src_sqlite(\u0026#34;example2.sqlite\u0026#34;, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;birdtracks\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.19.3 ## # [/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/r_large_data_files_handling/example2.sqlite] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  Remark that the dates are properly handled, by making sure the date representation inside SQlite is the converted string version.\n","href":"/tutorials/tutorials/r_large_data_files_handling/","title":"Reading large data files in R"},{"content":" library(googlesheets) library(dplyr) library(ggplot2) Setup The R package googlesheets provides the functionality to retrieve data from a google sheet. Once the registration as a user is done and the permissions are granted, it enables to read and write to google sheets. Initiation of the authentification can be done with the gs_auth command.\ngs_auth() Google will ask to grant the package the permission to access your drive. This token is saved to in a file .httr-oauth in your current working directory. Make sure this is not part of your version control system. However, as we want to be able to make the running independent from the user authentification or without needing an interactive environment. Therefore, we can store the token in a file and get the authentification from there. The code loads the token from the file if the file exists. Otherwise a new token is created and stored in the file.\nif (file_test(\u0026#34;-f\u0026#34;, \u0026#34;googlesheets_token.rds\u0026#34;)) { gs_auth(token = \u0026#34;googlesheets_token.rds\u0026#34;) } else { # first get the token, but not caching it in a local file token \u0026lt;- gs_auth(cache = FALSE) # save the token into a file in the current directory saveRDS(token, file = \u0026#34;googlesheets_token.rds\u0026#34;) } Make sure the token is savely stored within your project folder without sharing it or putting it into the version control history. If you need more power (e.g. necessity of integration services such as Travis CI), check the encryption options in this manual.\nOnce registered, an overview of your available google sheets is provided as follows, with the option to provide a (regular) expression to search for in the names:\ngs_ls(\u0026#34;tutorial_example\u0026#34;) ## # A tibble: 2 x 10 ## sheet_title author perm version updated sheet_key ws_feed ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 tutorial_e… stijn… rw old 2018-12-13 16:37:58 1O9W_c0A… https:… ## 2 tutorial_e… stijn… rw old 2018-12-13 16:36:56 124Od346… https:… ## # ... with 3 more variables: alternate \u0026lt;chr\u0026gt;, self \u0026lt;chr\u0026gt;, alt_key \u0026lt;chr\u0026gt;  I just want to load a google spreadsheet Consider you have the authentification. You know the name, URL or key of the spreadheet and want to read this in, well use one of the functions gs_title(), gs_url() or gs_key().\nAs an example, I want to work on the sheet called TraitsRedListSpecies. Finding the spreadheet is done as follows:\ngoose \u0026lt;- gs_title(\u0026#34;tutorial_example_spreadsheet_2\u0026#34;) ## Sheet successfully identified: \u0026quot;tutorial_example_spreadsheet_2\u0026quot;  An overview of the different sheets provides sufficient information to retrieve the dataset and work with them:\ngs_ws_ls(goose) ## [1] \u0026quot;Tbl1 Ringgegevens\u0026quot; \u0026quot;Tbl2 Koppels\u0026quot; \u0026quot;Tbl3 Waarnemingen\u0026quot; ## [4] \u0026quot;Tbl4 Nesten\u0026quot; \u0026quot;Tbl5 Nestcontroles\u0026quot; \u0026quot;Tbl6 Uitkipsucces\u0026quot; ## [7] \u0026quot;Tbl7 Kuikenoverleving\u0026quot; \u0026quot;Tbl8 Datums\u0026quot;  So, getting the BreedingBirds sheet into a native R object can be done by reading the specific worksheet:\ngoose_data \u0026lt;- goose %\u0026gt;% gs_read(ws = \u0026#34;Tbl1 Ringgegevens\u0026#34;) ## Accessing worksheet titled 'Tbl1 Ringgegevens'. ## Parsed with column specification: ## cols( ## Ringnummer = col_character(), ## Kleurring = col_character(), ## Soort = col_character(), ## Leeftijdscategorie = col_character(), ## Geslacht = col_character(), ## `Datum vangst` = col_character(), ## `Locatie vangst` = col_character(), ## Vangtype = col_character(), ## Gestorven = col_character() ## )  Inspecting the data of the sheet:\nclass(goose_data) ## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;  head(goose_data) ## # A tibble: 6 x 9 ## Ringnummer Kleurring Soort Leeftijdscatego… Geslacht `Datum vangst` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 B1634 162 Bran… Adult Vrouw 7/20/2000 ## 2 B3204 181 Bran… Adult \u0026lt;NA\u0026gt; 8/9/2009 ## 3 K23913 636 Bran… Adult \u0026lt;NA\u0026gt; 8/9/2009 ## 4 K46701 \u0026lt;NA\u0026gt; Bran… Adult Onbekend 7/2/2012 ## 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 K46702 \u0026lt;NA\u0026gt; Bran… Adult Onbekend 7/2/2012 ## # ... with 3 more variables: `Locatie vangst` \u0026lt;chr\u0026gt;, Vangtype \u0026lt;chr\u0026gt;, ## # Gestorven \u0026lt;chr\u0026gt;  We can now start working on the data, e.g. make a plot of the number of species in each location:\nggplot(data = goose_data %\u0026gt;% filter(!is.na(`Locatie vangst`)), aes(x = `Locatie vangst`)) + geom_bar() ","href":"/tutorials/tutorials/r_google_sheets/","title":"Read data from google sheet"},{"content":"","href":"/tutorials/authors/","title":"Authors"},{"content":"","href":"/tutorials/search/","title":"Search"},{"content":"","href":"/tutorials/tags/","title":"Tags"},{"content":"","href":"/tutorials/tags/api/","title":"api"},{"content":"","href":"/tutorials/tags/biodiversity/","title":"biodiversity"},{"content":"","href":"/tutorials/tags/ci/","title":"ci"},{"content":"","href":"/tutorials/tags/data/","title":"data"},{"content":"","href":"/tutorials/tags/database/","title":"database"},{"content":"","href":"/tutorials/categories/development/","title":"development"},{"content":"","href":"/tutorials/tags/development/","title":"development"},{"content":"","href":"/tutorials/tags/gbif/","title":"gbif"},{"content":"","href":"/tutorials/tags/gis/","title":"gis"},{"content":"","href":"/tutorials/tags/git/","title":"git"},{"content":"","href":"/tutorials/tags/github/","title":"github"},{"content":"","href":"/tutorials/tags/google/","title":"google"},{"content":"","href":"/tutorials/categories/installation/","title":"installation"},{"content":"","href":"/tutorials/tags/installation/","title":"installation"},{"content":"Check the administrator or user pages in function of the administrator rights on your computer.\n","href":"/tutorials/installation/","title":"Installation notes"},{"content":"","href":"/tutorials/categories/literature/","title":"literature"},{"content":"","href":"/tutorials/tags/literature/","title":"literature"},{"content":"","href":"/tutorials/tags/maps/","title":"maps"},{"content":"","href":"/tutorials/tags/markdown/","title":"markdown"},{"content":"","href":"/tutorials/tags/open-science/","title":"open science"},{"content":"","href":"/tutorials/tags/packages/","title":"packages"},{"content":"","href":"/tutorials/tags/pandoc/","title":"pandoc"},{"content":"","href":"/tutorials/categories/r/","title":"r"},{"content":"","href":"/tutorials/tags/r/","title":"r"},{"content":"","href":"/tutorials/tags/rstudio/","title":"rstudio"},{"content":"","href":"/tutorials/tags/spreadsheet/","title":"spreadsheet"},{"content":"","href":"/tutorials/categories/styleguide/","title":"styleguide"},{"content":"","href":"/tutorials/tags/styleguide/","title":"styleguide"},{"content":"","href":"/tutorials/tags/tidyverse/","title":"tidyverse"},{"content":"","href":"/tutorials/categories/version-control/","title":"version control"},{"content":"","href":"/tutorials/tags/version-control/","title":"version control"},{"content":"","href":"/tutorials/tags/webservice/","title":"webservice"},{"content":"","href":"/tutorials/tags/windows/","title":"windows"}]
